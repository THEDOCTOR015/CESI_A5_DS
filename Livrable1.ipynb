{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random as r\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.utils import Sequence, plot_model\n",
    "\n",
    "# Afficher le répertoire de travail actuel\n",
    "print(\"Répertoire de travail actuel :\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset depuis Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1: Monter Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Étape 2: Installer unrar (si nécessaire, dépend de l'environnement Colab actuel)\n",
    "!apt-get install unrar\n",
    "\n",
    "# Étape 3: Décompresser le fichier .rar\n",
    "!unrar x \"/content/drive/My Drive/Dataset_cesi.rar\" \"/content/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes et variables globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_NO_PHOTO_FOLDERS = ['Dataset_cesi/Painting', 'Dataset_cesi/Schematics', 'Dataset_cesi/Text', 'Dataset_cesi/Sketch']\n",
    "#PATH_NO_PHOTO_FOLDERS = ['Dataset_cesi/Schematics']\n",
    "PATH_PHOTO_FOLDER = 'Dataset_cesi/Photo'\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "CHANNELS = 3\n",
    "\n",
    "# L'ensemble doit être égale à 1\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.05\n",
    "assert TRAIN_RATIO + VAL_RATIO + TEST_RATIO  == 1\n",
    "\n",
    "# Hyperparamètres\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 2\n",
    "\n",
    "START_TRAIN = 0\n",
    "STOP_TRAIN = TRAIN_RATIO\n",
    "START_VAL = TRAIN_RATIO\n",
    "STOP_VAL = START_VAL + VAL_RATIO\n",
    "START_TEST = STOP_VAL\n",
    "STOP_TEST = START_TEST + TEST_RATIO\n",
    "\n",
    "print(f'distribution => train : [{START_TRAIN}:{STOP_TRAIN}] val : [{START_VAL}:{STOP_VAL}]  test : [{START_TEST}:{STOP_TEST}]')\n",
    "\n",
    "indices = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métrics du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution de la taille des images ( 80s )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_sizes(folder_path):\n",
    "    heights = []\n",
    "    widths = []\n",
    "    \n",
    "    # Parcourir le dossier et lire chaque image\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            with Image.open(img_path) as img:\n",
    "                width, height = img.size\n",
    "                widths.append(width)\n",
    "                heights.append(height)\n",
    "    \n",
    "    # Afficher les distributions des largeurs et hauteurs\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(widths, bins=20, color='blue', alpha=0.7)\n",
    "    plt.title('Distribution des largeurs')\n",
    "    plt.xlabel('Largeur')\n",
    "    plt.ylabel('Nombre d\\'images')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(heights, bins=20, color='green', alpha=0.7)\n",
    "    plt.title('Distribution des hauteurs')\n",
    "    plt.xlabel('Hauteur')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_image_sizes(PATH_PHOTO_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Informations sur les images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_info(folder):\n",
    "    images = os.listdir(folder)\n",
    "    length = len(images)\n",
    "    return length\n",
    "# Dossier photo\n",
    "photo_length = get_folder_info(PATH_PHOTO_FOLDER)\n",
    "print(f'Nombre d\\'images dans le dossier photo: {photo_length}')\n",
    "\n",
    "# Dossiers sans photo\n",
    "no_photo_len = 0\n",
    "no_photo_folders_len = []\n",
    "for folder in PATH_NO_PHOTO_FOLDERS:\n",
    "    no_photo_folder_len = get_folder_info(folder)\n",
    "    no_photo_len += no_photo_folder_len\n",
    "    no_photo_folders_len.append(no_photo_folder_len)\n",
    "print(f'Nombre d\\'images dans les dossiers sans photo: {no_photo_len}')\n",
    "for i in range(len(PATH_NO_PHOTO_FOLDERS)):\n",
    "    print(f'Nombre d\\'images dans le dossier {PATH_NO_PHOTO_FOLDERS[i]}: {no_photo_folders_len[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Générateur du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator(Sequence):\n",
    "    def _getshuffle(self, lenght, start, stop):\n",
    "        global indices\n",
    "        if len(indices) == 0 :\n",
    "            # On initialise les indices\n",
    "            indices = np.arange(lenght)\n",
    "            np.random.shuffle(indices)\n",
    "        return np.array(indices[start:stop])\n",
    "        \n",
    "    def __init__(self, ensemble, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Récupère le chemin de toutes les images d'un dossier\n",
    "        def find_paths(folder_path, label):\n",
    "            paths = []\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(folder_path, filename)\n",
    "                    paths.append(img_path)\n",
    "            labels = [label] * len(paths)\n",
    "            return paths, labels\n",
    "        \n",
    "        x_path, y = [], []\n",
    "        \n",
    "        # Chargement des chemins pour le dossier photo\n",
    "        temp_x_path, temp_y = find_paths(PATH_PHOTO_FOLDER, 1)\n",
    "        x_path += temp_x_path\n",
    "        y += temp_y\n",
    "        \n",
    "        # Chargement des chemins pour les dossier no photo\n",
    "        for path in PATH_NO_PHOTO_FOLDERS :\n",
    "            temp_x_path, temp_y = find_paths(path, 0)\n",
    "            x_path += temp_x_path\n",
    "            y += temp_y\n",
    "        \n",
    "        # Concaténation des arrays\n",
    "        lenght_dataset = len(y)\n",
    "        # Selection de l'ensemble\n",
    "        if ensemble == 'train' :\n",
    "            start = int(START_TRAIN * lenght_dataset)\n",
    "            stop = int(STOP_TRAIN * lenght_dataset)\n",
    "        elif ensemble == 'val' :\n",
    "            start = int(START_VAL * lenght_dataset)\n",
    "            stop = int(STOP_VAL * lenght_dataset)\n",
    "        elif ensemble == 'test' :\n",
    "            start = int(START_TEST * lenght_dataset)\n",
    "            stop = int(STOP_TEST * lenght_dataset)\n",
    "\n",
    "        # Shuffle des données via l'indice\n",
    "        self.indices = self._getshuffle(lenght_dataset, start, stop)\n",
    "        self.x_path, self.y = x_path, np.array(y)\n",
    "        \n",
    "        # Affichage des informations\n",
    "        print(f'Taille du générateur de l\\'ensemble {ensemble} = {len(self)}')\n",
    "        print(f'Nombre d\\'images dans le générateur = {len(self.indices)}')\n",
    "        count_photo_generator = np.sum(self.y[self.indices] == 1)\n",
    "        count_no_photo_generator = np.sum(self.y[self.indices] == 0)\n",
    "        print(f'Nombre de photos dans le générateur = {count_photo_generator}')\n",
    "        print(f'Nombre de non photos dans le générateur = {count_no_photo_generator}')\n",
    "              \n",
    "    def __getitem__(self, index):\n",
    "        start_index = index * BATCH_SIZE\n",
    "        stop_index = (index + 1 ) * BATCH_SIZE\n",
    "        chosen_indices = self.indices[start_index:stop_index]\n",
    "        \n",
    "        x, y = [], []\n",
    "        \n",
    "        # On récupère les images\n",
    "        for indice in chosen_indices :\n",
    "            indice_path = self.x_path[indice]\n",
    "            indice_label = self.y[indice]\n",
    "            with Image.open(indice_path) as img:\n",
    "                img = img.resize((WIDTH, HEIGHT)) # Avoir la même taille\n",
    "                img = img.convert('RGB') # Avoir 3 channels\n",
    "                x.append(img)\n",
    "            y.append(indice_label)\n",
    "        \n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Calcule le nombre de batch par epoch\n",
    "        return int(np.ceil(len(self.indices) / BATCH_SIZE))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle des indices\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "train_generator = DatasetGenerator('train', use_multiprocessing=True, workers=6)\n",
    "print('---------------------------------')\n",
    "val_generator = DatasetGenerator('val', use_multiprocessing=True, workers=6)\n",
    "print('---------------------------------')\n",
    "test_generator = DatasetGenerator('test', use_multiprocessing=True, workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choissisez un générateur\n",
    "generator = train_generator\n",
    "\n",
    "r_index = r.randint(0, len(generator) - 1)\n",
    "x, y = generator.__getitem__(r_index)\n",
    "print(f'x shape: {x.shape}, y shape: {y.shape}')\n",
    "r_index = r.randint(0, x.shape[0] - 1)\n",
    "plt.imshow(x[r_index])\n",
    "label = 'Photo' if y[r_index] == 1 else 'No photo'\n",
    "plt.title(label)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement d'un modèle préexistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model(\"drive/My Drive/Livrable1.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    layers.InputLayer(shape=(HEIGHT, WIDTH, CHANNELS)),\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', strides=2),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('leaky_relu'),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', strides=2),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('leaky_relu'),\n",
    "    layers.Conv2D(128, (3, 3), padding='same', strides=2),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('leaky_relu'),\n",
    "    layers.Conv2D(256, (3, 3), padding='same', strides=2),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('leaky_relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('leaky_relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "], name='photo_classifier')\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model architecture\n",
    "plot_model(model, to_file=f'Livrable1_{model.name}.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback d'early stopping\n",
    "early_callback = callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "# Cycle d'entrainement\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, callbacks=[early_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Courbe d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'Livrable1_{model.name}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "generator_type = input('Entrer l\\'ensemble de données à tester (train, val, test) : ')\n",
    "if generator_type == 'train' :\n",
    "    generator = train_generator\n",
    "elif generator_type == 'val' :\n",
    "    generator = val_generator\n",
    "elif generator_type == 'test' :\n",
    "    generator = test_generator\n",
    "res_pred = []\n",
    "res_true = []\n",
    "count = 0\n",
    "# Predit batch par batch\n",
    "for x, y in generator:\n",
    "    y_pred = model.predict(x, verbose=0)\n",
    "    y_pred = np.round(y_pred).flatten()\n",
    "    y_pred = y_pred.tolist()\n",
    "    res_pred += y_pred\n",
    "    res_true += y.tolist()\n",
    "    count += 1\n",
    "    print(f'Batch {count} / {len(generator)}')\n",
    "    if count == len(generator) :\n",
    "        break\n",
    "res_pred = np.array(res_pred)\n",
    "res_true = np.array(res_true)\n",
    "\n",
    "# Calcul de la matrice de confusion avec le titre de son ensemble\n",
    "cm = confusion_matrix(res_true, res_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "# Affichage de la matrice de confusion\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No photo', 'Photo'], yticklabels=['No photo', 'Photo'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix - {generator_type} Acc: {precision:.2f}, Rec: {recall:.2f}, F1: {f1:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test unitaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "generator_type = 'val'\n",
    "if generator_type == 'train' :\n",
    "    generator = train_generator\n",
    "elif generator_type == 'val' :\n",
    "    generator = val_generator\n",
    "elif generator_type == 'test' :\n",
    "    generator = test_generator\n",
    "\n",
    "# On récupère une image aléatoirement depuis le générateur\n",
    "r_index = r.randint(0, len(generator.indices) - 1)\n",
    "img_path = generator.x_path[generator.indices[r_index]]\n",
    "label = generator.y[generator.indices[r_index]]\n",
    "label = 'Photo' if label == 1 else 'No photo'\n",
    "\n",
    "# On charge l'image\n",
    "with Image.open(img_path) as img:\n",
    "    img = img.resize((WIDTH, HEIGHT)) # Avoir la même taille\n",
    "    img = img.convert('RGB') # Avoir 3 channels\n",
    "\n",
    "# On fait la prédiction\n",
    "img = np.array(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "prediction = model.predict(img)\n",
    "pred_label = 'Photo' if prediction[0][0] > 0.5 else 'No photo'\n",
    "\n",
    "# On affiche les résultats\n",
    "plt.imshow(img[0])\n",
    "plt.title(f'Label: {label}, Prediction: {pred_label}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
