{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import random as r\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes et variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASET_FOLDER = 'Dataset_cesi/Dataset/'\n",
    "RATIO_TRAIN = 0.8\n",
    "RATIO_VAL = 0.15\n",
    "RATIO_TEST = 0.05\n",
    "USE_MULTIPROCESSING = False\n",
    "WORKERS = 1 # si > 1, utilise threads si USE_MULTIPROCESSING = False, sinon utilise processus\n",
    "BATCH_SIZE = 4\n",
    "PATIENCE = 5\n",
    "EPOCHS = 100\n",
    "WIDTH = 640\n",
    "HEIGHT = 640\n",
    "CHANNELS = 3\n",
    "PROB_GAUSSIAN = 0.6\n",
    "PROB_PEPPER_SALT = 0.6\n",
    "PROB_OBJECTIF_DEFAULT = 0.5\n",
    "MODEL_NAME = 'livrable2.keras'\n",
    "\n",
    "indices = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de bruit\n",
    "| Type de Bruit          | Description                                                                                                                                                 |\n",
    "|------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Bruit Gaussien**     | Ajoute une variation aléatoire des intensités de pixels selon une distribution normale (gaussienne). Il est couramment observé dans les images numériques dues aux capteurs d'image et aux conditions d'éclairage. |\n",
    "| **Bruit Poivre et Sel**| Simule des pixels morts ou défectueux. Les pixels affectés deviennent blancs (sel) ou noirs (poivre). Commun dans les vieux films ou les capteurs défectueux. |\n",
    "| **Bruit de Défault d'objectif**   | Représente des défauts de capture ou de transmission, tels que les rayures sur une photo ou des erreurs de scan. Il ajoute des lignes ou des artéfacts visibles comme du flou local à travers l'image.                                    |\n",
    "| **Bruit Quantique**    | Provoqué par la fluctuation du nombre de photons détectés, typiquement dans des conditions de faible luminosité. Ce bruit est souvent modélisé comme un bruit de Poisson.                                               |\n",
    "| **Bruit de Speckle**   | Souvent présent dans les applications utilisant la lumière cohérente, comme les radars ou les ultrasons. Ce bruit peut être simulé en multipliant l'image par un bruit gaussien.                                        |\n",
    "\n",
    "Pour la suite de ce notebook, nous utiliserons les bruits Gaussien, Poivre et Sel, et de Défaut d'objectif.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_gaussian(image, mean=0, sigma=25):\n",
    "    gauss = np.random.normal(mean, sigma, image.shape).reshape(image.shape)\n",
    "    noisy_image = image + gauss\n",
    "    noisy_image = np.clip(noisy_image, 0, 255)\n",
    "    return noisy_image.astype(np.uint8)\n",
    "\n",
    "def add_noise_salt_pepper(image, salt_prob=0.008, pepper_prob=0.008):\n",
    "    num_salt = np.ceil(salt_prob * image.size)\n",
    "    num_pepper = np.ceil(pepper_prob * image.size)\n",
    "\n",
    "    # Ajout de sel\n",
    "    coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "              for i in image.shape]\n",
    "    image[coords[0], coords[1]] = 255\n",
    "\n",
    "    # Ajout de poivre\n",
    "    coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "              for i in image.shape]\n",
    "    image[coords[0], coords[1]] = 0\n",
    "\n",
    "    return image.astype(np.uint8)\n",
    "\n",
    "def add_noise_speckle(image, mean=0, sigma=0.1):\n",
    "    gauss = np.random.normal(mean, sigma, image.shape).reshape(image.shape)\n",
    "    noisy_image = image + image * gauss\n",
    "    noisy_image = np.clip(noisy_image, 0, 255)\n",
    "    return noisy_image.astype(np.uint8)\n",
    "\n",
    "def add_noice_objectif(image):\n",
    "    def add_noise_stripes(image, num_stripes_mean=2, num_stripes_std=1, color=(0, 0, 0)):\n",
    "        rows, cols, _ = image.shape\n",
    "        num_stripes = int(np.random.normal(num_stripes_mean, num_stripes_std))\n",
    "        for _ in range(num_stripes):\n",
    "            start = np.random.randint(0, cols - 1)\n",
    "            end = start + np.random.randint(1, 5)  # Largeur de la rayure\n",
    "            image[:, start:end] = color  # Rayure verticale noire\n",
    "        return image\n",
    "    \n",
    "    def add_local_blur(image, num_areas_mean=1, num_areas_std=1, area_size_mean=180, area_size_std=20, sigma_mean=10, sigma_std=2):\n",
    "        rows, cols, _ = image.shape\n",
    "        num_areas = int(np.random.normal(num_areas_mean, num_areas_std))\n",
    "        area_size = int(np.random.normal(area_size_mean, area_size_std))\n",
    "        sigma = np.random.normal(sigma_mean, sigma_std)\n",
    "        for _ in range(num_areas):\n",
    "            center_x = np.random.randint(0, cols - area_size)\n",
    "            center_y = np.random.randint(0, rows - area_size)\n",
    "            sub_img = image[center_y:center_y+area_size, center_x:center_x+area_size]\n",
    "            sub_img_blurred = cv2.GaussianBlur(sub_img, (7, 7), sigmaX=sigma, sigmaY=sigma)\n",
    "            image[center_y:center_y+area_size, center_x:center_x+area_size] = sub_img_blurred\n",
    "        return image\n",
    "    \n",
    "    def add_artifacts(image, num_artifacts_mean = 3, num_artifacts_std = 2, artifact_size_mean = 8, artifact_size_std = 3):\n",
    "        rows, cols, _ = image.shape\n",
    "        num_artifacts = int(np.random.normal(num_artifacts_mean, num_artifacts_std))\n",
    "        artifact_size = int(np.random.normal(artifact_size_mean, artifact_size_std))\n",
    "        for _ in range(num_artifacts):\n",
    "            center_x = np.random.randint(0, cols - artifact_size)\n",
    "            center_y = np.random.randint(0, rows - artifact_size)\n",
    "            color = np.random.randint(0, 256, (3,))\n",
    "            image[center_y:center_y+artifact_size, center_x:center_x+artifact_size] = color\n",
    "        return image\n",
    "    \n",
    "    # Ajout de rayures\n",
    "    image_stripes = add_noise_stripes(image)\n",
    "    # Ajout de flou local\n",
    "    image_blur = add_local_blur(image_stripes)\n",
    "    # Ajout d'artefacts\n",
    "    image_artifacts = add_artifacts(image_blur)\n",
    "    return image_artifacts\n",
    "\n",
    "# Charger l'image\n",
    "image = Image.open(\"exemple.jpg\")\n",
    "image = np.array(image)\n",
    "\n",
    "# Appliquer les différents bruits\n",
    "noisy_gaussian = add_noise_gaussian(image.copy())\n",
    "noisy_salt_pepper = add_noise_salt_pepper(image.copy())\n",
    "noisy_speckle = add_noise_speckle(image.copy())\n",
    "noisy_objectif = add_noice_objectif(image.copy())\n",
    "\n",
    "# Afficher les résultats\n",
    "fig, axs = plt.subplots(1, 4, figsize=(25, 5))\n",
    "axs[0].imshow(noisy_gaussian, cmap='gray')\n",
    "axs[0].set_title('Bruit Gaussien')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(noisy_salt_pepper, cmap='gray')\n",
    "axs[1].set_title('Bruit Poivre et Sel')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(noisy_speckle, cmap='gray')\n",
    "axs[2].set_title('Bruit de Speckle')\n",
    "axs[2].axis('off')\n",
    "\n",
    "axs[3].imshow(noisy_objectif, cmap='gray')\n",
    "axs[3].set_title('Bruit de Défaut d\\'objectif')\n",
    "axs[3].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métrics du dataset\n",
    "On prendra comme dimension d'image pour notre modèle 640x640 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_sizes(folder_path):\n",
    "    heights = []\n",
    "    widths = []\n",
    "    \n",
    "    # Parcourir le dossier et lire chaque image\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            with Image.open(img_path) as img:\n",
    "                width, height = img.size\n",
    "                widths.append(width)\n",
    "                heights.append(height)\n",
    "    \n",
    "    # Afficher les distributions des largeurs et hauteurs\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(widths, bins=20, color='blue', alpha=0.7)\n",
    "    plt.title('Distribution des largeurs')\n",
    "    plt.xlabel('Largeur')\n",
    "    plt.ylabel('Nombre d\\'images')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(heights, bins=20, color='green', alpha=0.7)\n",
    "    plt.title('Distribution des hauteurs')\n",
    "    plt.xlabel('Hauteur')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_image_sizes(PATH_DATASET_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator(Sequence):\n",
    "    def _getshuffle(self, lenght, start, stop):\n",
    "        global indices\n",
    "        if len(indices) == 0 :\n",
    "            # On initialise les indices\n",
    "            indices = np.arange(lenght)\n",
    "            np.random.shuffle(indices)\n",
    "        return np.array(indices[start:stop])\n",
    "    \n",
    "    def __init__(self, ensemble, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Récupère le chemin de toutes les images d'un dossier\n",
    "        def find_paths(folder_path):\n",
    "            paths = []\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(folder_path, filename)\n",
    "                    paths.append(img_path)\n",
    "            return paths\n",
    "        \n",
    "        self.y_path = find_paths(PATH_DATASET_FOLDER)\n",
    "        \n",
    "        # Créer un ensemble de données\n",
    "        if ensemble == 'train':\n",
    "            start = 0\n",
    "            stop = int(RATIO_TRAIN*len(self.y_path))\n",
    "        elif ensemble == 'val':\n",
    "            start = int(RATIO_TRAIN*len(self.y_path))\n",
    "            stop = int((RATIO_TRAIN+RATIO_VAL)*len(self.y_path))\n",
    "        elif ensemble == 'test':\n",
    "            start = int((RATIO_TRAIN+RATIO_VAL)*len(self.y_path))\n",
    "            stop = len(self.y_path)\n",
    "        # Shuffle des données via les indices\n",
    "        len_dataset = len(self.y_path)\n",
    "        self.indices = self._getshuffle(len_dataset, start, stop)\n",
    "        \n",
    "        # Affichage des informations\n",
    "        print(f\"Création du générateur pour l'ensemble {ensemble}\")\n",
    "        print(f\"Nombre d'images : {len(self.y_path)} / {len_dataset}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indices) / BATCH_SIZE))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        start_index = index * BATCH_SIZE\n",
    "        stop_index = (index + 1) * BATCH_SIZE\n",
    "        chosen_indices = self.indices[start_index:stop_index]\n",
    "        x,y = [],[]\n",
    "        \n",
    "        # On récupère les images et les labels\n",
    "        for indices in chosen_indices:\n",
    "            # Charger l'image\n",
    "            img = Image.open(self.y_path[indices])\n",
    "            img = img.resize((WIDTH, HEIGHT))\n",
    "            img = np.array(img)\n",
    "            img_noisy = img.copy()\n",
    "            # Ajouter du bruit aléatoirement\n",
    "            roll_gaussian = r.random()\n",
    "            roll_salt_pepper = r.random()\n",
    "            roll_objectif = r.random()\n",
    "            if roll_gaussian < PROB_GAUSSIAN:\n",
    "                img_noisy = add_noise_gaussian(img_noisy)\n",
    "            if roll_salt_pepper < PROB_PEPPER_SALT:\n",
    "                img_noisy = add_noise_salt_pepper(img_noisy)\n",
    "            if roll_objectif < PROB_OBJECTIF_DEFAULT:\n",
    "                img_noisy = add_noice_objectif(img_noisy)\n",
    "            x.append(img_noisy)\n",
    "            y.append(img)\n",
    "        x = np.array(x)\n",
    "        y = np.array(y) / 255.0 \n",
    "        return x, y\n",
    " \n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle des indices\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "train_generator = DatasetGenerator('train', use_multiprocessing=USE_MULTIPROCESSING, workers=WORKERS)\n",
    "print('---------------------------------')\n",
    "val_generator = DatasetGenerator('val', use_multiprocessing=USE_MULTIPROCESSING, workers=WORKERS)\n",
    "print('---------------------------------')\n",
    "test_generator = DatasetGenerator('test', use_multiprocessing=USE_MULTIPROCESSING, workers=WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choissisez un générateur\n",
    "generator = train_generator\n",
    "\n",
    "r_index = r.randint(0, len(generator) - 1)\n",
    "x, y = generator.__getitem__(r_index)\n",
    "print(f'x.shape={x.shape} y.shape={y.shape}')\n",
    "\n",
    "# Plot the results\n",
    "fig, axs = plt.subplots(2, BATCH_SIZE, figsize=(20, 5))\n",
    "for i in range(BATCH_SIZE):\n",
    "    axs[0, i].imshow(y[i], cmap='gray')\n",
    "    axs[0, i].axis('off')\n",
    "    axs[1, i].imshow(x[i], cmap='gray')\n",
    "    axs[1, i].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement d'un modèle pré-existant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MODEL_NAME)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_encoder():\n",
    "    inputs = layers.Input(shape=(WIDTH, HEIGHT, CHANNELS))\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2D(64, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(3, 3, strides=2, padding='same')(x)\n",
    "    x = layers.Activation('sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x, name='auto_encoder')\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def unet_auto_encoder():\n",
    "    def conv_block(x, filters, strides=1):\n",
    "        skip = layers.Conv2D(filters, 1, padding='same', strides=strides)(x)\n",
    "        x = layers.Conv2D(filters, 3, padding='same', strides=1)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "        x = layers.Conv2D(filters, 3, padding='same', strides=strides)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "        x = layers.Concatenate()([x, skip])\n",
    "        return x\n",
    "    \n",
    "    def transpose_conv_block(x, filters, strides=1):\n",
    "        skip = layers.Conv2DTranspose(filters, 1, padding='same', strides=strides)(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding='same', strides=1)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding='same', strides=strides)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "        x = layers.Concatenate()([x, skip])\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    input = layers.Input(shape=(WIDTH, HEIGHT, CHANNELS))\n",
    "    input = layers.Rescaling(1.0 / 255)(input)\n",
    "    \n",
    "    c1 = conv_block(input, 16, 2)\n",
    "    c2 = conv_block(c1, 32, 2)\n",
    "    c3 = conv_block(c2, 64, 2)\n",
    "    \n",
    "    p2 = transpose_conv_block(c3, 64, 2)\n",
    "    p2 = layers.Concatenate()([p2, c2])\n",
    "    p1 = transpose_conv_block(p2, 32, 2)\n",
    "    p1 = layers.Concatenate()([p1, c1])\n",
    "    p0 = transpose_conv_block(p1, 16, 2)\n",
    "    p0 = layers.Concatenate()([p0, input])\n",
    "    \n",
    "    output = layers.Conv2D(3, 1, padding='same', activation='sigmoid')(p0)\n",
    "    \n",
    "    model = Model(inputs=input, outputs=output, name='unet_auto_encoder')\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "model = unet_auto_encoder()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=f'Livrable2_{model.name}.jpg', show_shapes=True, show_layer_names=False, rankdir='TB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback d'early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=PATIENCE,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Boucle d'entraînement\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test unitaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choissisez un générateur\n",
    "generator = train_generator\n",
    "\n",
    "# Choix d'une image aléatoire\n",
    "r_index = r.randint(0, len(generator) - 1)\n",
    "x, y = generator.__getitem__(r_index)\n",
    "r_index = r.randint(0, len(x) - 1)\n",
    "x_sample = x[r_index]\n",
    "y_sample = y[r_index]\n",
    "\n",
    "# Prédiction\n",
    "y_pred = model.predict(np.expand_dims(x_sample, axis=0))[0]\n",
    "\n",
    "# Plot the results\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].imshow(y_sample)\n",
    "axs[0].set_title('Image originale')\n",
    "axs[0].axis('off')\n",
    "axs[1].imshow(x_sample)\n",
    "axs[1].set_title('Image bruitée')\n",
    "axs[1].axis('off')\n",
    "axs[2].imshow(y_pred)\n",
    "axs[2].set_title('Image restaurée')\n",
    "axs[2].axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
