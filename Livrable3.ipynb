{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce livrable concerne la dernière étape du traitement requis. L'objectif est de créer un réseau de neurones qui génère des légendes pour des photographies, en s'appuyant sur le dataset dataset MS COCO. Le réseau sera composé de deux parties, la partie CNN qui encode les images en un représentation interne, et le partie RNN utilise cette représentation pour prédire l'annotation séquence par séquence. Avant l'entraînement du modèle les images sont prétraitées par un CNN pré-entrainé pour la classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[insérer schéma v2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réalisation Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces imports ci-contre servent à construire un réseau de neurones capable de générer des légendes pour des photographies du dataset MS COCO. Les modules de TensorFlow et Keras sont utilisés pour définir et entraîner le modèle, avec une partie CNN pour encoder les images et une partie RNN pour générer des annotations mot par mot. Le modèle ResNet50, pré-entraîné, est importé pour prétraiter les images et extraire des caractéristiques visuelles pertinentes, facilitant ainsi la tâche de l'encodeur CNN. PIL est utilisé pour la gestion des images, et pycocotools pour manipuler le dataset COCO. Enfin, les bibliothèques de NLP comme nltk et Gensim permettent de traiter et de vectoriser les textes, nécessaires pour la partie RNN du réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models, losses, callbacks\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from pycocotools.coco import COCO\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from tensorflow.keras.models import load_model\n",
    "import sys\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(gpus)\n",
    "print(tf.__version__)\n",
    "\n",
    "print(\"Eager execution:\", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes, Variables et Fonctions Globales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant définit des constantes, des variables et des fonctions globales nécessaires à la création du modèle de génération de légendes d'images basé sur le dataset MS COCO. Les constantes permettent de configurer l'environnement d'entraînement (comme les chemins des fichiers de données, la taille des vecteurs textuels, ou les paramètres d'entraînement). Il initialise un modèle Word2Vec pour la vectorisation des légendes et fournit des fonctions pour trouver les mots ou vecteurs les plus proches dans selon le contexte. Des couches personnalisées pour le modèle, comme la Positional Encoding pour ajouter des informations de position aux séquences textuelles et la PaddingTruncatingLayer pour normaliser la longueur des séquences, sont également définies. Enfin, des classes de traitement d'images par patches facilitent l'extraction et l'encodage de blocs d'images, optimisant ainsi l'intégration des caractéristiques visuelles dans le réseau de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTDIR = 'annotations_trainval2014'\n",
    "DATADIR = 'train2014'\n",
    "CAPFILE = '{}/annotations/captions_{}.json'.format(ANNOTDIR, DATADIR)\n",
    "INSTANCEFILE = '{}/annotations/instances_{}.json'.format(ANNOTDIR, DATADIR)\n",
    "ALLOW_STOPWORD = True\n",
    "TEXT_VECTOR_SIZE = 512 #VOCAB_SIZE = 24918\n",
    "WORD2VEC_PATH = f'word2vec_captions_{TEXT_VECTOR_SIZE}.txt'\n",
    "MAX_LEN_SEQUENCE = 60 # 57 Obtenu via trainement des données\n",
    "PROCESS_IMAGE = False # True pour utiliser ResNet50 sinon False (standardisera les images entre 0 et 1)\n",
    "START_TOKEN = '<sos>'\n",
    "END_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "PADDING_TOKEN = '<pad>'\n",
    "coco_captions = COCO(CAPFILE)\n",
    "coco_instances = COCO(INSTANCEFILE)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "RATIO_TRAIN = 0.8\n",
    "RATIO_VAL = 0.15\n",
    "RATIO_TEST = 0.05\n",
    "PATIENCE = 3\n",
    "\n",
    "if os.path.exists(WORD2VEC_PATH):\n",
    "    try :\n",
    "        global VOCAB_SIZE, PADDING_INDEX, vec\n",
    "        vec = KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary=False)\n",
    "        print(f\"KeyedVectors loaded from {WORD2VEC_PATH}\")\n",
    "        PADDING_INDEX = vec.add_vector(PADDING_TOKEN, np.zeros(TEXT_VECTOR_SIZE))\n",
    "        print(f\"Padding index : {PADDING_INDEX}\")\n",
    "        VOCAB_SIZE = len(vec.index_to_key)\n",
    "        print(\"VOCAB_SIZE :\",VOCAB_SIZE)\n",
    "        \n",
    "    except e as Exception:\n",
    "        print(f\"Error loading KeyedVectors from {WORD2VEC_PATH} error : {e}\")\n",
    "        vec = None\n",
    "else:\n",
    "    print(f\"No model found at {WORD2VEC_PATH}\")\n",
    "\n",
    "assert RATIO_TRAIN + RATIO_VAL + RATIO_TEST == 1 # Vérification de la somme des ratios\n",
    "assert TEXT_VECTOR_SIZE == vec.vector_size # Vérification de la taille des vecteurs\n",
    "\n",
    "def find_closest_word(vector,):\n",
    "    # Cas pour le padding\n",
    "    if np.all(vector == 0):\n",
    "        return PADDING_TOKEN\n",
    "    # Calculer la similarité de cosine entre le vecteur donné et tous les vecteurs dans Word2Vec\n",
    "    similarities = cosine_similarity([vector], vec.vectors)[0] # 0 -> tuple (word, similarity) 0 -> word  PS : Fonction O(n) mais très optimisé (493 μs ± 4.53)\n",
    "    # Trouver l'index du vecteur le plus similaire\n",
    "    closest_index = similarities.argmax()\n",
    "    # Retourner le mot correspondant à cet index\n",
    "    return vec.index_to_key[closest_index]\n",
    "\n",
    "def find_closest_vector(vector, vec, printable=False):\n",
    "    # Calculer la similarité de cosine entre le vecteur donné et tous les vecteurs dans Word2Vec\n",
    "    similarities = cosine_similarity([vector], vec.vectors)[0]\n",
    "    if printable:\n",
    "        print(f'Similarities : {similarities}')\n",
    "    # Récupérer le vecteur le plus similaire\n",
    "    closest_vector = vec.vectors[similarities.argmax()]\n",
    "    return closest_vector\n",
    "\n",
    "def get_positional_encoding(max_seq_length, embed_size, scale=10000):\n",
    "    positional_encoding = np.array([\n",
    "        [pos / np.power(scale, 2 * (i//2) / embed_size) for i in range(embed_size)]\n",
    "        if pos != 0 else np.zeros(embed_size) for pos in range(max_seq_length)\n",
    "    ], dtype=np.float32)\n",
    "    positional_encoding[:, 0::2] = np.sin(positional_encoding[:, 0::2])  # dimensions 2i\n",
    "    positional_encoding[:, 1::2] = np.cos(positional_encoding[:, 1::2])  # dimensions 2i+1\n",
    "    return tf.cast(positional_encoding, dtype=tf.float32)\n",
    "\n",
    "### LAYERS CUSTOM ###\n",
    "\n",
    "class PositionalEncodingLayer(layers.Layer):\n",
    "    def __init__(self, max_seq_length, embed_size, scale=10000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embed_size = embed_size\n",
    "        self.positional_encoding = get_positional_encoding(max_seq_length, embed_size, scale=scale)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        seq_length = tf.shape(x)[1]\n",
    "        # Réduit positional_encoding à la longueur de la séquence réelle en cas de séquence plus courte que max_seq_length\n",
    "        pe = self.positional_encoding[:seq_length, :]\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Étend le masque pour qu'il ait la même dimension que x et pe\n",
    "            mask = tf.cast(mask, tf.float32)\n",
    "            mask = tf.expand_dims(mask, axis=-1)\n",
    "            # Utilise le masque pour annuler l'encoding sur les positions masquées\n",
    "            pe *= mask\n",
    "        \n",
    "        return x + pe\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"max_seq_length\": self.max_seq_length,\n",
    "            \"embed_size\": self.embed_size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class PaddingTruncatingLayer(layers.Layer):\n",
    "    def __init__(self, maxlen=MAX_LEN_SEQUENCE, padding_value=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.maxlen = maxlen\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Obtient la taille réelle des séquences\n",
    "        input_shape = tf.shape(inputs)\n",
    "\n",
    "        # Tronque les séquences si elles sont plus longues que maxlen\n",
    "        inputs = inputs[:, input_shape[1]-self.maxlen:, :]\n",
    "\n",
    "        # Calcule le padding nécessaire\n",
    "        padding_size = self.maxlen - tf.shape(inputs)[1]\n",
    "\n",
    "        # Crée un padding de taille [batch_size, padding_size, features_dim]\n",
    "        padding = tf.fill([input_shape[0], padding_size, input_shape[2]], self.padding_value)\n",
    "        padding = tf.cast(padding, tf.float32)\n",
    "\n",
    "        # Concatène le padding à l'input pour atteindre maxlen\n",
    "        out_tensor = tf.concat([inputs, padding], axis=1)\n",
    "        return tf.reshape(out_tensor, self.compute_output_shape(input_shape))\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.maxlen, input_shape[2])\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        num_patches = (images.shape[1] // self.patch_size) * (images.shape[2] // self.patch_size)\n",
    "        patches = tf.reshape(patches, [batch_size, num_patches, patch_dims])\n",
    "        return patches\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"patch_size\": self.patch_size}\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"num_patches\": self.num_patches, \"projection_dim\": self.projection.units}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code permet de visualiser et d'explorer les données du dataset en affichant une image aléatoire avec ses légendes associées. Il sélectionne d'abord un ID d'image au hasard parmi ceux disponibles dans les annotations d'instances, puis charge et affiche l'image correspondante avec matplotlib. Ensuite, il récupère les étiquettes de légendes liées à cette image et les affiche sous forme de texte. Cela permet de vérifier visuellement la correspondance entre les images et les descriptions textuelles, il s'agit d'une étape pour s'assurer que les données sont bien organisées et prêtes à être utilisées pour l'entraînement du modèle de captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner un ID d'image au hasard\n",
    "imgIds = coco_instances.getImgIds()\n",
    "print(f' Number of images found in instances :',len(imgIds))\n",
    "randomImgId = np.random.choice(imgIds)\n",
    "found_img = coco_instances.imgs[randomImgId]\n",
    "file_name = found_img['file_name']\n",
    "print(f\" Filename : {file_name}\")\n",
    "\n",
    "image = Image.open(f'{DATADIR}/{file_name}')\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Désactiver les axes, qui ne sont pas nécessaires pour l'affichage d'image\n",
    "plt.show()\n",
    "\n",
    "# Récupérer les IDs des annotations de légendes pour l'image sélectionnée\n",
    "annIds = coco_captions.getAnnIds(imgIds=randomImgId)\n",
    "# Charger les annotations\n",
    "anns = coco_captions.loadAnns(annIds)\n",
    "# Afficher les légendes\n",
    "print(\"Captions for the selected image:\")\n",
    "for ann in anns:\n",
    "    print(f\"- {ann['caption']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-Traitement pour Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie réalise le prétraitement des légendes d'images en vue de leur vectorisation avec Word2Vec. La fonction process_text nettoie chaque légende en supprimant les caractères non-alphabétiques, en convertissant le texte en minuscules et en retirant les stopwords si configuré. Des tokens de début et de fin (<sos>, <eos>) sont également ajoutés pour délimiter chaque séquence. Ensuite, le code parcourt toutes les images du dataset, extrait les légendes associées, les nettoie et les stocke dans une liste structurée. Aussi, on affiche des statistiques utiles sur les légendes (comme le nombre moyen de légendes par image et leur longueur maximale) pour mieux comprendre la structure des données avant l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de nettoyage de texte\n",
    "def process_text(text):\n",
    "    # Retirer les caractères non-alphabétiques et convertir en minuscules\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Retirer les stop words si besoin\n",
    "    if not ALLOW_STOPWORD:\n",
    "        tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
    "    # Ajouter les tokens de début et de fin\n",
    "    tokens.insert(0, START_TOKEN)  # Insérer le token de début en première position\n",
    "    tokens.append(END_TOKEN)  # Ajouter le token de fin\n",
    "    return tokens\n",
    "\n",
    "count_captions = 0\n",
    "count_invidual_captions = 0\n",
    "raw_captions = []\n",
    "for id in imgIds :\n",
    "    caption_ids = coco_captions.getAnnIds(imgIds=id)\n",
    "    captions_data = coco_captions.loadAnns(caption_ids)\n",
    "    captions = [process_text(caption['caption']) for caption in captions_data]\n",
    "    count_invidual_captions += len(captions)\n",
    "    count_captions += 1\n",
    "    raw_captions += captions # On aura donc raw_captions une liste de listes\n",
    "max_captions = max([len(raw_captions[i]) for i in range(len(raw_captions))])\n",
    "max_len_captions = max([len(raw_captions[i][j]) for i in range(len(raw_captions)) for j in range(len(raw_captions[i]))])\n",
    "print('Attention, statistique avec captions altérés (ajout des tokens de début et de fin)')\n",
    "print('count_captions :',count_captions)\n",
    "print('count_invidual_captions :',count_invidual_captions)\n",
    "print('mean number of caption per image :',count_invidual_captions/count_captions)\n",
    "print('max number of captions :',max_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner un modèle Word2Vec\n",
    "wordvec = Word2Vec(raw_captions, vector_size=TEXT_VECTOR_SIZE, window=4, min_count=1, workers=3, epochs=100)\n",
    "\n",
    "# Nombre de mots dans le vocabulaire\n",
    "vocab_size = len(wordvec.wv.key_to_index)\n",
    "print(f\"Nombre de mots dans le vocabulaire : {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sauvegarde de Word2Vec (données uniquement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec.wv.save_word2vec_format(WORD2VEC_PATH, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec = Word2Vec.load(WORD2VEC_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test unitaire de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = input('Quel mot souhaitez-vous avoir de similaire ? :')\n",
    "#START_TOKEN = '<sos>'\n",
    "#END_TOKEN = '<eos>'\n",
    "\n",
    "if word in vec.key_to_index:\n",
    "    similar_words = vec.most_similar(word)\n",
    "    print(\"Mots similaires à '{}':\".format(word))\n",
    "    for similar_word, similarity in similar_words:\n",
    "        print(f\"{similar_word}: {similarity:.4f}\")\n",
    "else:\n",
    "    # Si le mot n'est pas dans le vocabulaire, afficher un message d'erreur\n",
    "    print(\"Désolé, le mot '{}' n'est pas dans le vocabulaire.\".format(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Générateur de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des générateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit un générateur de données personnalisé, DatasetGenerator, pour préparer les lots (batches) d'images et de légendes nécessaires à l'entraînement du modèle de génération de légendes. En utilisant la classe Sequence de Keras, il permet de diviser le dataset  en ensembles d'entraînement, de validation et de test. Le générateur charge les images, les redimensionne, les normalise, et sélectionne aléatoirement des légendes associées. Les légendes sont nettoyées et converties en indices numériques via le modèle Word2Vec. Chaque légende est également \"coupée\" à un point aléatoire pour l'entraînement, où le modèle apprend à prédire le mot suivant. Les légendes tronquées sont ensuite remplies avec du padding pour la même longueur. Dans un dernier point, les images peuvent être prétraitées avec ResNet50 pour une normalisation avancée. Ce générateur optimise l'entraînement en fournissant des lots adaptés au modèle tout en mélangeant les données à la fin de chaque époque pour améliorer la généralisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator(Sequence):\n",
    "    def _getsplit(self, ensemble):\n",
    "        if ensemble == 'train':\n",
    "            start = 0\n",
    "            stop = int(RATIO_TRAIN * len(self.imgIds))\n",
    "        elif ensemble == 'val':\n",
    "            start = int(RATIO_TRAIN * len(self.imgIds))\n",
    "            stop = int((RATIO_TRAIN + RATIO_VAL) * len(self.imgIds))\n",
    "        elif ensemble == 'test':\n",
    "            start = int((RATIO_TRAIN + RATIO_VAL) * len(self.imgIds))\n",
    "            stop = len(self.imgIds)\n",
    "        return start, stop\n",
    "    \n",
    "    # Fonction de nettoyage de texte\n",
    "    def _clean_text(self,text):\n",
    "        # Retirer les caractères non-alphabétiques et convertir en minuscules\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        # Retirer les stop words si besoin\n",
    "        if not ALLOW_STOPWORD:\n",
    "            tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
    "        # Ajouter les tokens de début et de fin\n",
    "        tokens.insert(0, START_TOKEN)  # Insérer le token de début en première position\n",
    "        tokens.append(END_TOKEN)  # Ajouter le token de fin\n",
    "        return tokens\n",
    "    \n",
    "    def __init__(self, ensemble, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ensemble = ensemble\n",
    "        \n",
    "        # Créer une liste de tous les IDs d'images\n",
    "        self.imgIds = coco_instances.getImgIds()\n",
    "        start, stop = self._getsplit(ensemble)\n",
    "        self.ids = self.imgIds[start:stop]\n",
    "        self.captions_ids = { id : coco_captions.getAnnIds(imgIds=id) for id in self.ids }\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.ids) / BATCH_SIZE))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_ids = self.ids[index * BATCH_SIZE : (index + 1) * BATCH_SIZE]\n",
    "        batch_images = []\n",
    "        batch_captions = []\n",
    "        batch_labels = []\n",
    "        max_len_captions = 0\n",
    "        for id in batch_ids:\n",
    "            # Charger l'image\n",
    "            file_name = coco_instances.imgs[id]['file_name']\n",
    "            image = Image.open(f'{DATADIR}/{file_name}')\n",
    "            image = image.resize((224, 224))\n",
    "            image = image.convert('RGB')\n",
    "            image = np.array(image)\n",
    "            batch_images.append(image)\n",
    "            # Charger une légende aléatoire\n",
    "            caption_ids = self.captions_ids[id]\n",
    "            chosen_id = np.random.choice(caption_ids)\n",
    "            caption = coco_captions.anns[chosen_id]['caption'] # Accès directe car API buggée\n",
    "            caption = self._clean_text(caption)\n",
    "            r_index = np.random.randint(1, len(caption)) # On ne prend pas le token de début\n",
    "            caption_crop = caption[:r_index] # On crop la légende pour l'entrainement du modèle\n",
    "            caption_label = caption[r_index] # On garde le mot suivant pour la prédiction\n",
    "            caption_indexs = [ vec.key_to_index[caption_crop[i]] for i in range(len(caption_crop))]\n",
    "            caption_index_label = vec.key_to_index[caption_label]\n",
    "            len_caption = len(caption_indexs)\n",
    "            if len_caption > max_len_captions:\n",
    "                max_len_captions = len_caption\n",
    "            batch_captions.append(caption_indexs)\n",
    "            batch_labels.append(caption_index_label)\n",
    "        if PROCESS_IMAGE:\n",
    "            batch_images = preprocess_input(np.array(batch_images).copy())\n",
    "        else :\n",
    "            batch_images = np.array(batch_images) / 255.0\n",
    "        batch_captions = pad_sequences(batch_captions, maxlen=max_len_captions, padding='post', value=PADDING_INDEX, dtype='float32')\n",
    "        batch_labels = np.array(batch_labels)\n",
    "\n",
    "        return ((batch_images, batch_captions), batch_labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.ids = np.random.permutation(self.ids)\n",
    "\n",
    "train_generator = DatasetGenerator('train')\n",
    "val_generator = DatasetGenerator('val')\n",
    "test_generator = DatasetGenerator('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test unitaire du générateur de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on sélectionne un lot (batch) aléatoire d'images et de légendes d'entraînement, et vérifie la structure des données en affichant la forme des images, des légendes et des labels générés. Pour un test visuel, une image est choisie au hasard dans le lot, avec une légende partielle servant d'entrée et le mot suivant (le label) comme cible de prédiction. L'image est ensuite prétraitée et affichée avec la légende correspondante reconstituée à partir des indices numériques grâce au modèle Word2Vec. Ce test permet de s'assurer que le générateur produit des données cohérentes et prêtes pour l'entraînement, en facilitant la vérification visuelle des correspondances image-légende"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = train_generator\n",
    "# Récupérer un batch d'images et de légendes\n",
    "r_index = np.random.randint(len(generator))\n",
    "x, labels = generator.__getitem__(r_index-1)\n",
    "images, captions = x\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Captions shape: {captions.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "# Plot d'une des images avec sa légende\n",
    "r_index = np.random.randint(0, images.shape[0])\n",
    "selected_image = images[r_index]\n",
    "selected_caption = captions[r_index]\n",
    "selected_caption = np.array(selected_caption, dtype='int16')\n",
    "selected_label = vec.index_to_key[labels[r_index]]\n",
    "\n",
    "if PROCESS_IMAGE:\n",
    "    # On recentre les valeurs de l'image\n",
    "    selected_image = ( selected_image - np.min(selected_image) ) / ( np.max(selected_image) - np.min(selected_image) ) * 255\n",
    "    # On convertit l'image en RGB pour l'affichage\n",
    "    selected_image = selected_image.astype('uint8')\n",
    "    selected_image = selected_image[...,::-1]\n",
    "\n",
    "# Convertir les indices de la légende en mots\n",
    "selected_caption_words = [ vec.index_to_key[index] for index in selected_caption]\n",
    "selected_caption_str = ' '.join(selected_caption_words)\n",
    "# Affichage de l'image et de la légende\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(selected_image)\n",
    "plt.title(f\"(Input: {selected_caption_str}) (Label: {selected_label})\")\n",
    "plt.axis('off')  # Désactiver les axes pour une meilleure visibilité\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de la layer d'embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code teste la couche d'embedding, essentielle pour transformer des mots en vecteurs numériques dans le cadre de l'entraînement du modèle. La fonction test_embedding prend un mot en entrée et vérifie s'il fait partie du vocabulaire. Si oui, elle récupère l'indice du mot et son vecteur d'embedding dans le modèle Word2Vec. Ensuite, un modèle Keras simple est créé avec une couche d'embedding initialisée à l'aide des vecteurs pré-entraînés de Word2Vec. Le modèle prédit l'embedding du mot donné, et le code compare ce résultat avec l'embedding original extrait directement de Word2Vec. L'objectif est de vérifier que la couche d'embedding de TensorFlow produit des vecteurs identiques aux embeddings pré-entraînés, garantissant que l'initialisation du modèle est correcte. Ce test permet de s'assurer que l'intégration des embeddings dans le réseau est fidèle aux données originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embedding(word):\n",
    "    # On test la layer d'embedding de tf\n",
    "    if word not in vec.key_to_index:\n",
    "        print(f\"Le mot '{word}' n'est pas dans le vocabulaire.\")\n",
    "        return\n",
    "    word_index = vec.key_to_index[word]\n",
    "    embedding = vec.get_vector(word)\n",
    "    print(f\"Index du mot '{word}' dans le vocabulaire : {word_index}\")\n",
    "    print(f\"Embedding du mot '{word} (wordvec)' : {embedding[0:5]}..\")\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(1,)))\n",
    "    model.add(layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False))\n",
    "    model.summary()\n",
    "    # Test de l'embedding\n",
    "    embedded_word = model.predict(np.array([[word_index]]))\n",
    "    print(f\"Embedding du mot '{word}' (calculé) : {embedded_word[0][0][0:5]}..\")\n",
    "    \n",
    "    print(\"Les deux embeddings sont-ils égaux ? :\", np.allclose(embedding, embedded_word[0][0]))\n",
    "\n",
    "test_embedding(input('Quel mot souhaitez-vous tester ? :'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de la layer de positionnal encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code teste la couche de Positional Encoding (PE), qui ajoute des informations sur la position des mots dans une séquence, permettant au modèle de comprendre l'ordre des mots. La fonction vérifie d'abord si la couche fonctionne correctement sur un batch réel de données provenant du générateur, puis sur un masque de zéros pour voir comment elle gère des séquences entièrement vides. En cas de succès, le résultat de l'encodage est visualisé, illustrant la manière dont la PE encode les positions dans des dimensions paires et impaires. Cela permet de valider que la couche de PE fonctionne comme prévu dans différentes situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pe(length, scale):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(None,)))\n",
    "    model.add(layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False))\n",
    "    model.add(PositionalEncodingLayer(max_seq_length=MAX_LEN_SEQUENCE, embed_size=TEXT_VECTOR_SIZE, scale=scale))\n",
    "    # Test de l'encoding avec un batch de données\n",
    "    r_index = np.random.randint(len(train_generator))\n",
    "    x, y = train_generator.__getitem__(r_index)\n",
    "    x0, x1 = x\n",
    "    try :\n",
    "        model.predict(x1, verbose=0)\n",
    "        print(\"PE fonctionne correctement sur un batch de données.\")\n",
    "    except e as Exception:\n",
    "        print(f\"Erreur lors de l'application de PE sur un batch de données : {e}\")\n",
    "    \n",
    "    # Test de l'encoding avec un masque de zéros\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(None,TEXT_VECTOR_SIZE)))\n",
    "    model.add(PositionalEncodingLayer(max_seq_length=MAX_LEN_SEQUENCE, embed_size=TEXT_VECTOR_SIZE, scale=scale))\n",
    "    try :\n",
    "        zero_mask = np.zeros((BATCH_SIZE, length, TEXT_VECTOR_SIZE), dtype='float32')\n",
    "        pe = model.predict(zero_mask, verbose=0)\n",
    "        print(\"PE fonctionne correctement avec un masque de zéros.\")\n",
    "    except e as Exception:\n",
    "        print(f\"Erreur lors de l'application de PE avec un masque de zéros : {e}\")\n",
    "    \n",
    "    if pe is not None:\n",
    "        # Affichage de l'encoding\n",
    "        plt.figure(figsize=(25, 5))\n",
    "        pe0 = pe[0]\n",
    "        # Plot des valeurs pair et impair\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        ax[0].plot(pe0[:, 0::2])\n",
    "        ax[0].set_title('Dimensions paires')\n",
    "        ax[1].plot(pe0[:, 1::2])\n",
    "        ax[1].set_title('Dimensions impaires')\n",
    "        plt.show()\n",
    "\n",
    "test_pe(length=60, scale=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de la layer de Padding Truncage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code teste la couche de Padding et de Truncation, qui est utilisée pour ajuster la longueur des séquences d'entrée afin qu'elles aient toutes une taille uniforme, facilitant ainsi le traitement par le modèle. La fonction commence par créer un modèle séquentiel, ajoute une couche d'embedding, puis applique la couche de padding et truncation. Ensuite, un batch aléatoire de données est récupéré, et une légende sélectionnée est passée à travers le modèle pour générer une prédiction. Le code affiche la légende réelle et celle générée par le modèle, permettant ainsi de comparer les deux. Enfin, il visualise l'image associée à la légende pour contextualiser le résultat. Cette approche est essentielle pour s'assurer que les séquences sont correctement préparées pour l'apprentissage profond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_padding():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(None,)))\n",
    "    model.add(layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False))\n",
    "    model.add(PaddingTruncatingLayer(maxlen=MAX_LEN_SEQUENCE))\n",
    "    model.summary()\n",
    "    # Test de la couche de padding avec un batch de données\n",
    "    r_index = np.random.randint(len(train_generator))\n",
    "    x, y = train_generator.__getitem__(r_index)\n",
    "    x0, x1 = x\n",
    "    # Choix aléatoire d'un index dans le batch\n",
    "    r_index = np.random.randint(x1.shape[0])\n",
    "    selected_caption = x1[r_index]\n",
    "    selected_image = x0[r_index]\n",
    "    # On recentre les valeurs de l'image\n",
    "    selected_image = ( selected_image - np.min(selected_image) ) / ( np.max(selected_image) - np.min(selected_image) ) * 255\n",
    "    # On convertit l'image en RGB pour l'affichage\n",
    "    selected_image = np.array(selected_image)\n",
    "    selected_image = selected_image.astype('uint8')\n",
    "    selected_image = selected_image[...,::-1]\n",
    "    selected_caption = np.expand_dims(selected_caption, axis=0)\n",
    "    res = None\n",
    "    res = model.predict(selected_caption, verbose=0)[0] # x1 : batch de captions  selected_caption : caption sélectionnée\n",
    "    true_caption = [ vec.index_to_key[int(selected_caption[0][i])] for i in range(len(selected_caption[0])) ]\n",
    "    true_caption_str = ' '.join(true_caption)\n",
    "    print(f'True caption : {true_caption_str}')\n",
    "    # On cherche les mot donc les vecteurs sont similaires depuis \"res\"\n",
    "    selected_caption_words = [ find_closest_word(res[i]) for i in range(len(res))]\n",
    "    selected_caption_words_str = ' '.join(selected_caption_words)\n",
    "    print(f'Model caption : {selected_caption_words_str}')\n",
    "    print(f\"Lenght of true caption : {len(true_caption)}\")\n",
    "    print(f\"Lenght of Model caption : {len(selected_caption_words)}\")\n",
    "    # On plot le résultat du padding\n",
    "    plt.figure(figsize=(25, 5))\n",
    "    plt.imshow(selected_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "test_padding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de la layer d'attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code teste une couche d'attention, un composant clé dans les modèles d'apprentissage profond, notamment dans le traitement du langage naturel et la vision par ordinateur. La fonction test_attention définit un modèle qui prend deux entrées de formes différentes, représentant généralement des séquences ou des représentations de caractéristiques. En appliquant la couche d'attention à ces deux entrées, le modèle produit une sortie qui met en évidence les relations pertinentes entre elles, facilitant ainsi la capture des dépendances contextuelles. La méthode summary() affiche la structure du modèle, permettant aux utilisateurs de vérifier la configuration des couches et de s'assurer que l'architecture est correctement définie pour intégrer l'attention. Cela est essentiel pour optimiser la performance des modèles lors de l'apprentissage sur des tâches complexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attention(shape1, shape2):\n",
    "    input1 = layers.Input(shape=shape1)\n",
    "    input2 = layers.Input(shape=shape2)\n",
    "    attention = layers.Attention()([input1, input2])\n",
    "    model = Model(inputs=[input1, input2], outputs=attention)\n",
    "    model.summary()\n",
    "\n",
    "test_attention((64, 256), (63, 196))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de la layer Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code teste la couche d'attention multi-tête, un élément essentiel des architectures modernes comme les Transformers, qui améliorent considérablement la capacité des modèles à gérer des informations complexes. La fonction test_multi_head_attention crée un modèle qui prend deux entrées de dimensions spécifiées, permettant à la couche d'attention multi-tête de calculer des représentations enrichies en intégrant simultanément plusieurs sous-espaces d'attention. Cela permet au modèle de se concentrer sur différentes parties de l'entrée et de capturer des dépendances contextuelles variées, ce qui est particulièrement utile dans des tâches telles que la traduction automatique ou la classification de séquences. L'appel à model.summary() permet de vérifier la configuration du modèle et de s'assurer que la couche d'attention est intégrée correctement, garantissant ainsi une mise en œuvre efficace de cette technique d'attention avancée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multi_head_attention(shape1, shape2, key_dim=128, num_heads=1):\n",
    "    input1 = layers.Input(shape=shape1)\n",
    "    input2 = layers.Input(shape=shape2)\n",
    "    # Création de la couche d'attention\n",
    "    attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(input1, input2)\n",
    "    model = Model(inputs=[input1, input2], outputs=attention)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "test_multi_head_attention((60,256),(49,256),1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de la layer de Patching et de Patching Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code teste les couches de découpage (patching) et d'encodage de patch dans le contexte des architectures de traitement d'images, telles que les modèles Vision Transformer. La fonction reconstruct_image permet de reconstruire une image complète à partir de ses patches, démontrant ainsi comment les informations peuvent être récupérées après avoir été segmentées. La fonction test_patches évalue le modèle de découpage en appliquant une opération de patching sur un batch d'images, permettant de visualiser les patches générés et leur reconstruction par rapport à l'image originale. De plus, test_patches_encoding ajoute une étape d'encodage après le découpage, transformant les patches en représentations de dimension inférieure, ce qui est crucial pour des tâches d'apprentissage en profondeur. Ensemble, ces tests assurent que les couches fonctionnent correctement et permettent d'explorer comment les modèles peuvent traiter les images de manière plus efficace en capturant les relations spatiales à travers des patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_image(patches, original_shape, patch_size):\n",
    "    # Reconstruction des patches en une image complète\n",
    "    batch_size = patches.shape[0]\n",
    "    num_patches_per_row = original_shape[1] // patch_size\n",
    "    num_patches_per_col = original_shape[2] // patch_size\n",
    "\n",
    "    reconstructed_image = tf.reshape(patches, [batch_size, num_patches_per_row, num_patches_per_col, patch_size, patch_size, original_shape[3]])\n",
    "    reconstructed_image = tf.transpose(reconstructed_image, [0, 1, 3, 2, 4, 5])\n",
    "    reconstructed_image = tf.reshape(reconstructed_image, [batch_size, original_shape[1], original_shape[2], original_shape[3]])\n",
    "    return reconstructed_image\n",
    "\n",
    "def test_patches(generator=train_generator):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(224, 224, 3)))\n",
    "    model.add(Patches(patch_size=14))\n",
    "    model.summary()\n",
    "\n",
    "    # Obtenir un batch d'images\n",
    "    r_index = np.random.randint(len(generator))\n",
    "    x, y = generator.__getitem__(r_index)\n",
    "    images, _ = x\n",
    "    \n",
    "    patches = model.predict(images)\n",
    "    print(f\"Shape des patches : {patches.shape}\")\n",
    "\n",
    "    reconstructed = reconstruct_image(patches, (1, 224, 224, 3), 14)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(images[0])\n",
    "    plt.title(\"Image Originale\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(reconstructed[0])\n",
    "    plt.title(\"Image Reconstruite\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def test_patches_encoding():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(224, 224, 3)))\n",
    "    model.add(Patches(patch_size=14))\n",
    "    model.add(PatchEncoder(num_patches=16*16, projection_dim=256))\n",
    "    model.summary()\n",
    "\n",
    "    # Obtenir un batch d'images\n",
    "    r_index = np.random.randint(len(train_generator))\n",
    "    x, y = train_generator.__getitem__(r_index)\n",
    "    images, _ = x\n",
    "    patches = model.predict(images)\n",
    "\n",
    "test_patches()\n",
    "\n",
    "test_patches_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1 du modèle (caption_modelv1)\n",
    "\n",
    "Cette version initiale se concentre sur une approche simple, utilisant un ResNet50 pré-entraîné pour le traitement d'images et un LSTM pour le traitement de texte. Le ResNet50 agit comme un extracteur de caractéristiques où ses couches sont gelées pour conserver les poids appris. Les caractéristiques de l'image sont aplaties et passées à travers une couche dense pour réduire la dimensionnalité, tandis que l'entrée de texte est masquée pour gérer les longueurs de séquence variables et traitée via un LSTM. La sortie combine les caractéristiques de l'image et du texte, suivies d'une normalisation par lots et de couches denses pour produire une sortie finale qui utilise l'activation ReLU. La fonction de perte utilisée est la similarité cosinus, qui mesure l'angle entre la sortie et le vecteur cible, plutôt que la distance euclidienne, offrant une robustesse dans les tâches de similarité.\n",
    "\n",
    "#### Version 2 du modèle (caption_modelv2)\n",
    "\n",
    "La version 2 améliore l'architecture précédente en introduisant quelques modifications visant à améliorer les performances. Les caractéristiques de l'image sont traitées via une couche dense plus grande (512 unités) et suivies d'une normalisation par lots et d'une activation Leaky ReLU, qui permet d'éviter la disparition des gradients. L'entrée de texte est traitée différemment ; au lieu de LSTM directement, une couche d'intégration est ajoutée, permettant une meilleure représentation des mots. La sortie de la couche LSTM est ajustée pour renvoyer la dernière séquence, en se concentrant sur le contexte final du texte. La couche de sortie passe à une activation softmax pour la classification multi-classes, reflétant le passage d'une sortie basée sur la régression à une sortie catégorielle. Les performances du modèle sont évaluées à l'aide d'une entropie croisée catégorielle éparse.\n",
    "\n",
    "Cette version est celle retenue pour son intégration dans ce projet car elle répond à tout les attendus ; cependant, nous avons cherché à continuer l'amélioration du modèle au travers de nombreuses versions décrites en suivant.\n",
    "\n",
    "#### Version 3 du modèle (caption_modelv3)\n",
    "\n",
    "Cette version introduit une approche étendue en ajoutant la gestion des séquences pour les données d'image. La principale innovation ici est l'extension des données d'image pour créer une dimension de séquence, ce qui permet la combinaison de caractéristiques d'image avec des caractéristiques de texte à chaque étape temporelle. Cela se fait à l'aide de tf.tile, ce qui permet au modèle d'utiliser les deux modalités de manière plus intégrée. Les caractéristiques de texte utilisent toujours des incorporations et des LSTM, mais maintenant l'entrée combinée est traitée via un LSTM bidirectionnel. L'architecture met l'accent sur la capture du contexte à la fois de l'image et du texte tout au long de leurs séquences, améliorant potentiellement la génération de légendes plus sensibles au contexte.\n",
    "\n",
    "#### Version 4 du modèle (caption_modelv4)\n",
    "\n",
    "La version 4 du modèle implémente un mécanisme d'auto-attention pour le traitement du texte, s'éloignant de la seule dépendance aux LSTM. Elle utilise une couche d'attention multi-têtes pour capturer plus efficacement les dépendances dans le texte. De plus, elle conserve la couche dense pour les caractéristiques de l'image, mais ajoute une structure LSTM plus complexe pour le traitement du texte, en exploitant plusieurs couches bidirectionnelles pour améliorer l'extraction des caractéristiques. La sortie est à nouveau traitée via des couches denses, le modèle expérimentant différentes configurations de masquage causal dans les couches d'attention, ce qui peut affecter la façon dont les futurs tokens sont traités par rapport aux tokens actuels.\n",
    "\n",
    "#### Version 5 du modèle (caption_modelv5)\n",
    "\n",
    "Cette version s'appuie sur les concepts d'auto-attention de la version 4, mais intègre l'attention croisée entre les caractéristiques de l'image et du texte. Elle traite d'abord le texte via un mécanisme d'auto-attention, puis l'aligne avec les données d'image étendues pour l'attention croisée. Cette méthode permet au modèle de prendre en compte les caractéristiques de l'image lors de la génération du texte, améliorant ainsi la relation entre les modalités visuelles et textuelles. Les couches LSTM sont conservées pour capturer les dépendances séquentielles dans le texte, mais l'architecture permet des interactions plus nuancées entre les caractéristiques de l'image et du texte, visant à affiner la qualité des légendes générées.\n",
    "\n",
    "#### Version 6 du modèle (caption_modelv6)\n",
    "\n",
    "Dans la version 6, l'architecture évolue en encapsulant le traitement de texte dans une fonction de bloc réutilisable qui applique une attention multi-tête suivie de LSTM bidirectionnels. Cette approche modulaire facilite l'expérimentation avec différentes configurations et améliore la clarté de l'architecture. Le modèle utilise toujours l'auto-attention et maintient un flux de traitement structuré pour le texte. L'architecture reste similaire à la version 5 mais met l'accent sur la modularité et l'utilisation d'une technique de traitement de texte raffinée pour améliorer l'apprentissage à partir de données d'image et de texte.\n",
    "\n",
    "#### Version 7 du modèle (caption_modelv7)\n",
    "\n",
    "Ce modèle passe à une architecture de transformateur, exploitant pleinement les capacités des mécanismes d'attention. Il construit une couche de décodeur de transformateur qui traite à la fois les entrées d'image et de texte via des mécanismes d'attention. Le modèle introduit le concept de codage de patch pour les images, lui permettant de capturer des détails précis à partir des données visuelles. En utilisant des couches qui implémentent l'attention multi-têtes et les réseaux de rétroaction, l'architecture permet une compréhension contextuelle et une interaction plus approfondies entre les modalités. L'utilisation d'un masque causal garantit que le modèle respecte la nature séquentielle de la génération de texte, une caractéristique essentielle pour produire des légendes cohérentes.\n",
    "\n",
    "#### Version 8 du modèle (caption_modelv8)\n",
    "\n",
    "La version finale affine encore davantage l'architecture du transformateur en mettant en œuvre un processus d'auto-attention étendu. Cette version met l'accent sur la modularité en définissant un bloc transformateur pour l'auto-attention et l'attention croisée, ce qui permet une plus grande flexibilité dans la conception de l'architecture. Le modèle s'appuie sur un Vision Transformer (ViT) pré-entraîné pour l'extraction des caractéristiques de l'image, améliorant ainsi la qualité du traitement des données visuelles. Chaque bloc transformateur est conçu pour apprendre efficacement à partir de relations complexes au sein des données, à la fois au sein du texte et entre les modalités d'image et de texte. Cette version vise des performances supérieures grâce à une structure plus profonde et plus flexible capable de gérer des dépendances complexes entre les caractéristiques de l'image et les légendes générées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_modelv1():\n",
    "    '''\n",
    "    inputs :\n",
    "        image : (batch_size, 224, 224, 3)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        output : (batch_size, TEXT_VECTOR_SIZE)\n",
    "    results :\n",
    "        cosine similarity loss : -0.48\n",
    "    '''\n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    image_data = layers.Dense(256)(x)  # Nouvelle couche dense pour les caractéristiques\n",
    "    \n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,TEXT_VECTOR_SIZE))\n",
    "    x = layers.Masking(mask_value=0.0)(text_input) # Extrèmement important\n",
    "    text_data = layers.LSTM(512)(x)\n",
    "    context = layers.Concatenate()([image_data, text_data])\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Dense(2024)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    output = layers.Dense(TEXT_VECTOR_SIZE, activation='relu')(context)\n",
    "    \n",
    "    model = Model(inputs=(base_model.input, text_input), outputs=[output], name='caption_modelv1')\n",
    "    cosinus_loss = losses.CosineSimilarity()\n",
    "    model.compile(loss=cosinus_loss, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def caption_modelv2():\n",
    "    '''\n",
    "    inputs :\n",
    "        image : (batch_size, 224, 224, 3)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        output : (batch_size, TEXT_VECTOR_SIZE)\n",
    "    results :\n",
    "        sparse_categorical_crossentropy : 2.6\n",
    "        sparse_categorical_crossentropy : 2.9   # Version lourde\n",
    "        sparse_categorical_crossentropy : 2.55 # V2\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False # True pour fine-tuning ou si beaucoup de mémoire disponible\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    image_data = layers.Activation('leaky_relu')(x)\n",
    "\n",
    "    \n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,))\n",
    "    x = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False)(text_input)\n",
    "    x = layers.Masking(mask_value=0.0)(x) # Extrèmement important    \n",
    "    x = layers.LSTM(1024, return_sequences=False)(x) # return_sequences = True pour obtenir une sortie pour chaque mot\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    text_data = layers.Activation('leaky_relu')(x)\n",
    "    context = layers.Concatenate()([image_data, text_data])\n",
    "    context = layers.Dropout(0.1)(context)\n",
    "    output = layers.Dense(VOCAB_SIZE, activation='softmax')(context)\n",
    "    \n",
    "    model = Model(inputs=(base_model.input, text_input), outputs=[output], name='caption_modelv2')\n",
    "    loss = losses.sparse_categorical_crossentropy\n",
    "    model.compile(loss=loss, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def caption_modelv3():\n",
    "    '''\n",
    "    inputs :\n",
    "        image : (batch_size, 224, 224, 3)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        output : (batch_size, TEXT_VECTOR_SIZE)\n",
    "    results :\n",
    "        sparse_categorical_crossentropy : 5.2137\n",
    "    '''\n",
    "    \n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False # True pour fine-tuning ou si beaucoup de mémoire disponible\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    image_data = layers.Dense(256)(x)  # Nouvelle couche dense pour les caractéristiques\n",
    "    image_data_expanded = tf.expand_dims(image_data, 1)  # Ajoute une dimension de séquence\n",
    "    \n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,))\n",
    "    image_data_tiled = tf.tile(image_data_expanded, [1, tf.shape(text_input)[1], 1])  # Réplique le long de la dimension de séquence\n",
    "    text_features  = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False)(text_input)\n",
    "    text_features  = layers.Masking(mask_value=0.0)(text_features) # Extrèmement important\n",
    "    combined_features = layers.Concatenate(axis=-1)([text_features, image_data_tiled])\n",
    "    context = layers.Bidirectional(layers.LSTM(1024))(combined_features) # return_sequences = True pour obtenir une sortie pour chaque mot\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    context = layers.Dense(256)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    output = layers.Dense(VOCAB_SIZE, activation='softmax')(context)\n",
    "    \n",
    "    model = Model(inputs=(base_model.input, text_input), outputs=[output], name='caption_modelv3')\n",
    "    loss = losses.sparse_categorical_crossentropy\n",
    "    model.compile(loss=loss, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def caption_modelv4():\n",
    "    '''\n",
    "    inputs :\n",
    "        image : (batch_size, 224, 224, 3)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        output : (batch_size, TEXT_VECTOR_SIZE)\n",
    "    results :\n",
    "        sparse_categorical_crossentropy : 2.98     # self-attention avec causal_mask=False sans PE\n",
    "        sparse_categorical_crossentropy : 2.88     # self-attention avec causal_mask=True sans PE\n",
    "        sparse_categorical_crossentropy : 3.17     # self-attention avec causal_mask=True avec PE\n",
    "        sparse_categorical_crossentropy : 3.47        # self-attention avec causal_mask=True sans PE et avec modèle lourd (head=64 LSTM 1024-512-256 + 2x Dense 256)\n",
    "    '''\n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False # True pour fine-tuning ou si beaucoup de mémoire disponible\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    image_data = layers.Dense(256)(x)  # Nouvelle couche dense pour les caractéristiques\n",
    "\n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,))\n",
    "    text_features  = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False)(text_input)\n",
    "    #text_features = PositionalEncodingLayer(max_seq_length=MAX_LEN_SEQUENCE, embed_size=TEXT_VECTOR_SIZE, scale=100)(text_features)\n",
    "    text_features  = layers.Masking(mask_value=0.0)(text_features) # Extrèmement important\n",
    "    \n",
    "    # Self-Attention mechanism\n",
    "    self_attention_text = layers.MultiHeadAttention(num_heads=64, key_dim=TEXT_VECTOR_SIZE)(text_features, text_features)\n",
    "    self_attention_text.use_causal_mask = True\n",
    "    self_attention_text = layers.Add()([text_features, self_attention_text])\n",
    "    self_attention_text = layers.LayerNormalization()(self_attention_text)\n",
    "    self_attention_text = layers.Activation('leaky_relu')(self_attention_text)\n",
    "\n",
    "    text_data = layers.Bidirectional(layers.LSTM(1024, return_sequences=True))(self_attention_text)\n",
    "    text_data = layers.BatchNormalization()(text_data)\n",
    "    text_data = layers.Activation('leaky_relu')(text_data)\n",
    "    text_data = layers.Bidirectional(layers.LSTM(512, return_sequences=True))(text_data)\n",
    "    text_data = layers.BatchNormalization()(text_data)\n",
    "    text_data = layers.Activation('leaky_relu')(text_data)\n",
    "    text_data = layers.Bidirectional(layers.LSTM(256))(text_data)\n",
    "    text_data = layers.BatchNormalization()(text_data)\n",
    "    text_data = layers.Activation('leaky_relu')(text_data)\n",
    "    context = layers.Concatenate()([image_data, text_data])\n",
    "    context = layers.Dense(256)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    context = layers.Dense(256)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    output = layers.Dense(VOCAB_SIZE, activation='softmax')(context)\n",
    "\n",
    "    model = Model(inputs=(base_model.input, text_input), outputs=[output], name='caption_modelv4')\n",
    "    loss = losses.sparse_categorical_crossentropy\n",
    "    model.compile(loss=loss, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def caption_modelv5():  \n",
    "    '''\n",
    "    inputs :\n",
    "        image : (batch_size, 224, 224, 3)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        output : (batch_size, TEXT_VECTOR_SIZE)\n",
    "    results :\n",
    "        sparse_categorical_crossentropy : 4.16\n",
    "    '''\n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False # True pour fine-tuning ou si beaucoup de mémoire disponible\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    image_data = layers.Dense(256)(x)  # Nouvelle couche dense pour les caractéristiques\n",
    "\n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,))\n",
    "    text_features  = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False)(text_input)\n",
    "    #text_features = PositionalEncodingLayer(max_seq_length=MAX_LEN_SEQUENCE, embed_size=TEXT_VECTOR_SIZE, scale=100)(text_features)\n",
    "    text_features  = layers.Masking(mask_value=0.0)(text_features) # Extrèmement important\n",
    "    \n",
    "    # Self-Attention mechanism\n",
    "    self_attention_text = layers.MultiHeadAttention(num_heads=64, key_dim=TEXT_VECTOR_SIZE)(text_features, text_features)\n",
    "    self_attention_text.use_causal_mask = True\n",
    "    self_attention_text = layers.Add()([text_features, self_attention_text])\n",
    "    self_attention_text = layers.LayerNormalization()(self_attention_text)\n",
    "    self_attention_text = layers.Activation('leaky_relu')(self_attention_text)\n",
    "\n",
    "    # Prepare image data for cross-attention by repeating it to match text sequence length\n",
    "    seq_length = tf.shape(text_features)[1]  # Get the sequence length of text features\n",
    "    image_data_expanded = tf.expand_dims(image_data, 1)  # Expand dims to simulate sequence length\n",
    "    image_features_for_attention = tf.tile(image_data_expanded, [1, seq_length, 1])  # Tile across the sequence length\n",
    "\n",
    "    # Cross-Attention mechanism\n",
    "    cross_attention_text = layers.MultiHeadAttention(num_heads=64, key_dim=TEXT_VECTOR_SIZE)(self_attention_text, image_features_for_attention)\n",
    "    cross_attention_text = layers.Add()([self_attention_text, cross_attention_text])\n",
    "    cross_attention_text = layers.LayerNormalization()(cross_attention_text)\n",
    "    cross_attention_text = layers.Activation('leaky_relu')(cross_attention_text)\n",
    "\n",
    "    text_data = layers.Bidirectional(layers.LSTM(1024, return_sequences=True))(self_attention_text)\n",
    "    text_data = layers.BatchNormalization()(text_data)\n",
    "    text_data = layers.Activation('leaky_relu')(text_data)\n",
    "    text_data = layers.Bidirectional(layers.LSTM(512, return_sequences=True))(text_data)\n",
    "    text_data = layers.BatchNormalization()(text_data)\n",
    "    text_data = layers.Activation('leaky_relu')(text_data)\n",
    "    text_data = layers.Bidirectional(layers.LSTM(256))(text_data)\n",
    "    text_data = layers.BatchNormalization()(text_data)\n",
    "    text_data = layers.Activation('leaky_relu')(text_data)\n",
    "    context = layers.Concatenate()([image_data, text_data])\n",
    "    context = layers.Dense(256)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    output = layers.Dense(VOCAB_SIZE, activation='softmax')(context)\n",
    "\n",
    "    model = Model(inputs=(base_model.input, text_input), outputs=[output], name='caption_modelv5')\n",
    "    loss = losses.sparse_categorical_crossentropy\n",
    "    model.compile(loss=loss, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def caption_modelv6():\n",
    "    def text_block(x, n_head, d_lstm, d_model=TEXT_VECTOR_SIZE, return_sequences=True):\n",
    "        x = layers.MultiHeadAttention(num_heads=n_head, key_dim=d_model)(x, x)  # Self-attention\n",
    "        x.use_causal_mask = True # Mask des tokens futurs\n",
    "        x = layers.Add()([x, x])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = layers.Activation('leaky_relu')(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(d_lstm, return_sequences=return_sequences))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('leaky_relu')(x)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    inputs :\n",
    "        image : (batch_size, 224, 224, 3)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        output : (batch_size, TEXT_VECTOR_SIZE)\n",
    "    results :\n",
    "        sparse_categorical_crossentropy : 3.6\n",
    "\n",
    "    '''\n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False # True pour fine-tuning ou si beaucoup de mémoire disponible\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    image_data = layers.Dense(256)(x)  # Nouvelle couche dense pour les caractéristiques\n",
    "\n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,))\n",
    "    text_features  = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False)(text_input)\n",
    "    #text_features = PositionalEncodingLayer(max_seq_length=MAX_LEN_SEQUENCE, embed_size=TEXT_VECTOR_SIZE, scale=100)(text_features)\n",
    "    text_features  = layers.Masking(mask_value=0.0)(text_features) # Extrèmement important\n",
    "\n",
    "    text_data = text_block(text_features, 64, 1024)\n",
    "    text_data = text_block(text_data, 32, 512)\n",
    "    text_data = text_block(text_data, 16, 256, return_sequences=False)\n",
    "\n",
    "    context = layers.Concatenate()([image_data, text_data])\n",
    "    context = layers.Dense(256)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    context = layers.Dense(256)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    output = layers.Dense(VOCAB_SIZE, activation='softmax')(context)\n",
    "\n",
    "    model = Model(inputs=(base_model.input, text_input), outputs=[output], name='caption_modelv6')\n",
    "    loss = losses.sparse_categorical_crossentropy\n",
    "    model.compile(loss=loss, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def caption_modelv7():\n",
    "    def transformer_decoder_layer(query, key_value, key_dim, num_heads, dff, dropout=0.1,use_causal_mask=False):\n",
    "        \"\"\"\n",
    "        inputs :\n",
    "            query : (batch_size, query_seq_len, dim)\n",
    "            key_value : (batch_size, key_value_seq_len, dim)\n",
    "            key_dim : dimension des clés et valeurs dans la couche d'attention\n",
    "            num_heads : nombre de têtes dans la couche d'attention\n",
    "            dff : multiplieur pour la dimension des couches cachées dans le feed-forward network\n",
    "            dropout : taux de dropout\n",
    "            use_causal_mask : booléen pour utiliser un masque causal dans la couche d'attention\n",
    "        outputs :\n",
    "        \"\"\"\n",
    "        # Multi-Head Attention (utilise causal mask pour respecter l'ordre des mots dans le texte généré)\n",
    "        attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(query, key_value)\n",
    "        attention.use_causal_mask = use_causal_mask\n",
    "        attention = layers.Dropout(dropout)(attention)\n",
    "        attention = layers.Add()([attention, query])\n",
    "        attention = layers.LayerNormalization(epsilon=1e-6)(attention)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        ffn_output = layers.Dense(dff*key_dim, activation='leaky_relu')(attention)\n",
    "        ffn_output = layers.Dense(key_dim)(ffn_output)\n",
    "        ffn_output = layers.Dropout(dropout)(ffn_output)\n",
    "        ffn_output = layers.Add()([ffn_output, attention])\n",
    "        ffn_output = layers.LayerNormalization(epsilon=1e-6)(ffn_output)\n",
    "        return ffn_output\n",
    "    \n",
    "    GLOBAL_DIM = 512\n",
    "    global_dff = 2\n",
    "    global_num_heads = 12\n",
    "\n",
    "    # Image processing\n",
    "    image_input = layers.Input(shape=(224, 224, 3))\n",
    "    x = Patches(patch_size=14)(image_input)\n",
    "    x = PatchEncoder(num_patches=16*16, projection_dim=GLOBAL_DIM)(x)\n",
    "    x = transformer_decoder_layer(x, x, GLOBAL_DIM, num_heads=global_num_heads, dff=global_dff)\n",
    "    image_data = transformer_decoder_layer(x, x, GLOBAL_DIM, num_heads=global_num_heads, dff=global_dff)\n",
    "    \n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,))\n",
    "    text_features = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False)(text_input)\n",
    "    text_features = PaddingTruncatingLayer()(text_features)\n",
    "    text_features = PositionalEncodingLayer(max_seq_length=60, embed_size=TEXT_VECTOR_SIZE, scale=500)(text_features)\n",
    "\n",
    "    # Transformer Decoder\n",
    "    x = transformer_decoder_layer(text_features, text_features, GLOBAL_DIM, num_heads=global_num_heads, dff=global_dff, use_causal_mask=True)\n",
    "    x = transformer_decoder_layer(x,x, GLOBAL_DIM, num_heads=global_num_heads, dff=global_dff, use_causal_mask=True)\n",
    "    x = transformer_decoder_layer(x,x, GLOBAL_DIM, num_heads=global_num_heads, dff=global_dff, use_causal_mask=True)\n",
    "\n",
    "    x = transformer_decoder_layer(x, image_data, GLOBAL_DIM, num_heads=global_num_heads, dff=global_dff)\n",
    "    x = transformer_decoder_layer(x, image_data, GLOBAL_DIM, num_heads=global_num_heads, dff=global_dff)\n",
    "\n",
    "\n",
    "    decoder_output = layers.Lambda(lambda x: x[:, -1, :])(x)\n",
    "\n",
    "    # Final output layer\n",
    "    output = layers.Dense(VOCAB_SIZE, activation='softmax')(decoder_output)\n",
    "\n",
    "    model = Model(inputs=(image_input, text_input), outputs=[output], name='caption_modelv7')\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "def caption_modelv8():\n",
    "\n",
    "    def transformer_block(x, num_heads, projection_dim, ff_dim, dropout, x2=None, use_causal_mask=False):\n",
    "        # Normalisation et Multi-Head Attention\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x2 = x1 if x2 is None else x2\n",
    "        attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim\n",
    "        )(x1, x2)\n",
    "        attention.use_causal_mask = use_causal_mask\n",
    "        x2 = layers.Add()([attention, x])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "        # Normalisation et Feed-Forward\n",
    "        x3 = layers.Dense(units=ff_dim, activation='gelu')(x3)\n",
    "        x3 = layers.Dropout(dropout)(x3)\n",
    "        x3 = layers.Dense(units=projection_dim, activation='gelu')(x3)\n",
    "        x3 = layers.Dropout(dropout)(x3)\n",
    "        final = layers.Add()([x3, x2])\n",
    "        return final\n",
    "\n",
    "    # Image processing\n",
    "    total_vit = load_model('VIT_v1_2210.keras', custom_objects={'Patches': Patches, 'PatchEncoder': PatchEncoder})\n",
    "    features_layer = total_vit.get_layer('layer_normalization_96').output\n",
    "    vit_model = Model(inputs=total_vit.input, outputs=features_layer)\n",
    "    vit_model.trainable = False\n",
    "    image_input = vit_model.input\n",
    "    image_data = vit_model.output\n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,))\n",
    "    text_features = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False)(text_input)\n",
    "    text_features = PositionalEncodingLayer(max_seq_length=60, embed_size=TEXT_VECTOR_SIZE, scale=500)(text_features)\n",
    "\n",
    "    # Transformer Decoder\n",
    "    num_self_attentions_blocks = 8\n",
    "    self_attention_heads = 8\n",
    "    self_attention_projection_dim = TEXT_VECTOR_SIZE\n",
    "    self_attention_ff_dim = 4*self_attention_projection_dim\n",
    "    self_attention_dropout = 0.0\n",
    "\n",
    "    num_cross_attentions_blocks = 4\n",
    "    cross_attention_heads = 8\n",
    "    cross_attention_projection_dim = 128+32\n",
    "    cross_attention_ff_dim = 4*cross_attention_projection_dim\n",
    "    cross_attention_dropout = 0.0\n",
    "\n",
    "\n",
    "\n",
    "    for _ in range(num_self_attentions_blocks):\n",
    "        # Causal Self-Attention block\n",
    "        text_features = transformer_block(\n",
    "            text_features,\n",
    "            num_heads=self_attention_heads,\n",
    "            projection_dim=self_attention_projection_dim,\n",
    "            ff_dim=self_attention_ff_dim,\n",
    "            dropout=self_attention_dropout,\n",
    "            use_causal_mask=True\n",
    "        )\n",
    "\n",
    "    # Projection pour la cross-attention\n",
    "    context = layers.Dense(cross_attention_projection_dim)(text_features)\n",
    "\n",
    "    for _ in range(num_cross_attentions_blocks):\n",
    "        # Cross-Attention block\n",
    "        context = transformer_block(\n",
    "            context,\n",
    "            num_heads=cross_attention_heads,\n",
    "            projection_dim=cross_attention_projection_dim,\n",
    "            ff_dim=cross_attention_ff_dim,\n",
    "            dropout=cross_attention_dropout,\n",
    "            x2=image_data\n",
    "        )\n",
    "    \n",
    "    # Output\n",
    "    context = layers.LayerNormalization(epsilon=1e-6)(context)\n",
    "    #context = layers.GlobalAveragePooling1D()(context) # Moyenne sur les séquences\n",
    "    context = layers.Flatten()(context) # On aplatit le tenseur\n",
    "    #context = layers.Lambda(lambda x: x[:, -1, :])(context) # On prend le dernier élément de la séquence\n",
    "    output = layers.Dense(VOCAB_SIZE, activation='softmax')(context)\n",
    "\n",
    "    model = Model(inputs=(image_input, text_input), outputs=[output], name='caption_modelv8')\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = caption_modelv8()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement d'un modèle pré-existant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('Livrable3_caption_modelv2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=f'Livrable3_{model.name}.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=PATIENCE, \n",
    "                                         restore_best_weights=True)\n",
    "\n",
    "checkpoint_path = f'checkpoints/{model.name}'\n",
    "checkpoint_path = checkpoint_path + '-{epoch:04d}.keras'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data=val_generator,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'Livrable3_{model.name}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test unitaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il comprend deux versions de la fonction caption_model_inference: la première, caption_model_inferencev1, utilise une approche déterministe pour prédire des légendes à partir d'un vecteur d'image et d'un modèle LSTM, tandis que la seconde, caption_model_inferencev2, introduit une notion d'échantillonnage probabiliste via la température pour diversifier les résultats. Ces fonctions prennent en entrée une image prétraitée et un modèle de traitement de langage, et produisent une légende descriptive qui peut être utilisée dans des applications de vision par ordinateur, comme la recherche d'images ou l'accessibilité pour les personnes malvoyantes. La visualisation des résultats avec Matplotlib permet également d'évaluer visuellement les performances du modèle en affichant les images avec leurs légendes générées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_model_inferencev1(model,vec, image):\n",
    "    \"\"\"\n",
    "    inputs :\n",
    "        model : model keras\n",
    "        word2vec : model word2vec\n",
    "        image : (batch_size, 224,224,3) (RGB) (0,255)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        caption : (batch_size, 1, TEXT_VECTOR_SIZE)\n",
    "    \"\"\"\n",
    "    start_index = vec.key_to_index[START_TOKEN]\n",
    "    start_vector = vec.vectors[start_index]\n",
    "    processed_image = preprocess_input(image.copy())\n",
    "    caption = np.expand_dims([start_vector], axis=0)\n",
    "    caption_words = ''\n",
    "    \n",
    "    while True :\n",
    "        model.reset_states() # Reset les états de la LSTM\n",
    "        caption_vector = np.array(caption)\n",
    "        result = model.predict([np.array([processed_image]), caption_vector], verbose=0)[0]\n",
    "        result_vector = find_closest_vector(result, vec)\n",
    "\n",
    "        if np.array_equal(result_vector, vec[END_TOKEN]):\n",
    "            break\n",
    "        else:\n",
    "            caption_words += find_closest_word(result_vector, vec) + ' '\n",
    "            result_vector = np.expand_dims(result_vector, axis=0)\n",
    "            caption = np.concatenate([caption, [result_vector]], axis=1)\n",
    "        if len(caption_words.split()) >= MAX_LEN_SEQUENCE :\n",
    "            break\n",
    "    return caption_words\n",
    "\n",
    "def caption_model_inferencev2(model,vec, image, temperature=1e-5):\n",
    "    \"\"\"\n",
    "    inputs :\n",
    "        model : model keras\n",
    "        word2vec : model word2vec\n",
    "        image : image (batch_size, 224, 224, 3) (RGB) (0,255)\n",
    "        caption : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        caption : (batch_size, VOCAB_SIZE)\n",
    "    \"\"\"\n",
    "    start_index = vec.key_to_index[START_TOKEN]\n",
    "    processed_image = preprocess_input(image.copy())\n",
    "    caption = np.expand_dims([start_index], axis=0)\n",
    "    caption_words = ''\n",
    "    \n",
    "    while True :\n",
    "        model.reset_states() # Reset les états de la LSTM\n",
    "        caption_tensor = np.array(caption)\n",
    "        logits = model.predict([np.array([processed_image]), caption_tensor], verbose=0)[0]\n",
    "        probabilities = tf.nn.softmax(logits / temperature).numpy()  # Appliquer la température\n",
    "        index_result = np.random.choice(np.arange(len(probabilities)), p=probabilities)  # Échantillonnage\n",
    "\n",
    "\n",
    "        if index_result == vec.key_to_index[END_TOKEN]:\n",
    "            break\n",
    "        else:\n",
    "            # On print le % de confiance du token <eos> pour voir si le modèle est confiant\n",
    "            #print(f' % de confiance pour le mot <eos> : {result[vec.key_to_index[END_TOKEN]]}')\n",
    "            caption_words += vec.index_to_key[index_result] + ' '\n",
    "            result_tensor = np.expand_dims(index_result, axis=0)\n",
    "            caption = np.concatenate([caption, [result_tensor]], axis=1)\n",
    "        if len(caption_words.split()) >= MAX_LEN_SEQUENCE :\n",
    "            break\n",
    "    return caption_words\n",
    "\n",
    "# Choix du générateur\n",
    "generator = val_generator\n",
    "# Test du modèle\n",
    "r_index = np.random.randint(len(generator))\n",
    "x, _ = generator.__getitem__(r_index)\n",
    "images, _ = x\n",
    "r_index = np.random.randint(0, images.shape[0]-1)\n",
    "selected_image = images[r_index]\n",
    "# On recentre les valeurs de l'image\n",
    "selected_image = ( selected_image - np.min(selected_image) ) / ( np.max(selected_image) - np.min(selected_image) ) * 255\n",
    "# On convertit l'image en RGB pour l'affichage\n",
    "selected_image = np.array(selected_image)\n",
    "selected_image = selected_image.astype('uint8')\n",
    "selected_image = selected_image[...,::-1]\n",
    "\n",
    "# Prédiction de la légende\n",
    "predicted_caption = caption_model_inferencev2(model, vec, selected_image)\n",
    "\n",
    "# Plot de l'image avec la légende prédite\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(selected_image)\n",
    "plt.title(predicted_caption)\n",
    "plt.axis('off')  # Désactiver les axes pour une meilleure visibilité\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
