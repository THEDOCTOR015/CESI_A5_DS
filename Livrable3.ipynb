{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from pycocotools.coco import COCO\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes et Variables Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTDIR = 'annotations_trainval2014'\n",
    "DATADIR = 'train2014'\n",
    "CAPFILE = '{}/annotations/captions_{}.json'.format(ANNOTDIR, DATADIR)\n",
    "INSTANCEFILE = '{}/annotations/instances_{}.json'.format(ANNOTDIR, DATADIR)\n",
    "ALLOW_STOPWORD = True\n",
    "WORD2VEC_PATH = 'word2vec_captions.model'\n",
    "#MAX_LEN_SEQUENCE = 80 # 57 Obtenu via trainement des données\n",
    "TEXT_VECTOR_SIZE = 100 #VOCAB_SIZE = 24918\n",
    "IMAGE_VECTOR_SIZE = 128\n",
    "START_TOKEN = '<sos>'\n",
    "END_TOKEN = '<eos>'\n",
    "coco_captions = COCO(CAPFILE)\n",
    "coco_instances = COCO(INSTANCEFILE)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "RATIO_TRAIN = 0.8\n",
    "RATIO_VAL = 0.15\n",
    "RATIO_TEST = 0.05\n",
    "\n",
    "if os.path.exists(WORD2VEC_PATH):\n",
    "    wordvec = Word2Vec.load(WORD2VEC_PATH)\n",
    "    print(f\"Model loaded from {WORD2VEC_PATH}\")\n",
    "else:\n",
    "    print(f\"No model found at {WORD2VEC_PATH}\")\n",
    "\n",
    "assert RATIO_TRAIN + RATIO_VAL + RATIO_TEST == 1 # Vérification de la somme des ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sélectionner un ID d'image au hasard\n",
    "imgIds = coco_instances.getImgIds()\n",
    "print(f' Number of images found in instances :',len(imgIds))\n",
    "randomImgId = np.random.choice(imgIds)\n",
    "found_img = coco_instances.imgs[randomImgId]\n",
    "file_name = found_img['file_name']\n",
    "print(f\" Filename : {file_name}\")\n",
    "\n",
    "image = Image.open(f'{DATADIR}/{file_name}')\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Désactiver les axes, qui ne sont pas nécessaires pour l'affichage d'image\n",
    "plt.show()\n",
    "\n",
    "# Récupérer les IDs des annotations de légendes pour l'image sélectionnée\n",
    "annIds = coco_captions.getAnnIds(imgIds=randomImgId)\n",
    "# Charger les annotations\n",
    "anns = coco_captions.loadAnns(annIds)\n",
    "# Afficher les légendes\n",
    "print(\"Captions for the selected image:\")\n",
    "for ann in anns:\n",
    "    print(f\"- {ann['caption']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-Traitement pour Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de nettoyage de texte\n",
    "def process_text(text):\n",
    "    # Retirer les caractères non-alphabétiques et convertir en minuscules\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Retirer les stop words si besoin\n",
    "    if not ALLOW_STOPWORD:\n",
    "        tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
    "    # Ajouter les tokens de début et de fin\n",
    "    tokens.insert(0, START_TOKEN)  # Insérer le token de début en première position\n",
    "    tokens.append(END_TOKEN)  # Ajouter le token de fin\n",
    "    return tokens\n",
    "\n",
    "count_captions = 0\n",
    "count_invidual_captions = 0\n",
    "raw_captions = []\n",
    "for id in imgIds :\n",
    "    caption_ids = coco_captions.getAnnIds(imgIds=id)\n",
    "    captions_data = coco_captions.loadAnns(caption_ids)\n",
    "    captions = [process_text(caption['caption']) for caption in captions_data]\n",
    "    count_invidual_captions += len(captions)\n",
    "    count_captions += 1\n",
    "    raw_captions += captions # On aura donc raw_captions une liste de listes\n",
    "max_captions = max([len(raw_captions[i]) for i in range(len(raw_captions))])\n",
    "max_len_captions = max([len(raw_captions[i][j]) for i in range(len(raw_captions)) for j in range(len(raw_captions[i]))])\n",
    "print('Attention, statistique avec captions altérés (ajout des tokens de début et de fin)')\n",
    "print('count_captions :',count_captions)\n",
    "print('count_invidual_captions :',count_invidual_captions)\n",
    "print(' mean number of caption per image :',count_invidual_captions/count_captions)\n",
    "print(' max number of captions :',max_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner un modèle Word2Vec\n",
    "wordvec = Word2Vec(raw_captions, vector_size=TEXT_VECTOR_SIZE, window=4, min_count=1, workers=3, sg=1, epochs=100)\n",
    "\n",
    "# Nombre de mots dans le vocabulaire\n",
    "vocab_size = len(wordvec.wv.key_to_index)\n",
    "print(f\"Nombre de mots dans le vocabulaire : {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sauvegarde de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec.save(WORD2VEC_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec = Word2Vec.load(WORD2VEC_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test unitaire de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = input('Quel mot souhaitez-vous avoir de similaire ? :')\n",
    "#START_TOKEN = '<sos>'\n",
    "#END_TOKEN = '<eos>'\n",
    "\n",
    "if word in wordvec.wv.key_to_index:\n",
    "    similar_words = wordvec.wv.most_similar(word)\n",
    "    print(\"Mots similaires à '{}':\".format(word))\n",
    "    for similar_word, similarity in similar_words:\n",
    "        print(f\"{similar_word}: {similarity:.4f}\")\n",
    "else:\n",
    "    # Si le mot n'est pas dans le vocabulaire, afficher un message d'erreur\n",
    "    print(\"Désolé, le mot '{}' n'est pas dans le vocabulaire.\".format(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Générateur de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des générateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator(Sequence):\n",
    "    def _getsplit(self, ensemble):\n",
    "        if ensemble == 'train':\n",
    "            start = 0\n",
    "            stop = int(RATIO_TRAIN * len(self.imgIds))\n",
    "        elif ensemble == 'val':\n",
    "            start = int(RATIO_TRAIN * len(self.imgIds))\n",
    "            stop = int((RATIO_TRAIN + RATIO_VAL) * len(self.imgIds))\n",
    "        elif ensemble == 'test':\n",
    "            start = int((RATIO_TRAIN + RATIO_VAL) * len(self.imgIds))\n",
    "            stop = len(self.imgIds)\n",
    "        return start, stop\n",
    "    \n",
    "    # Fonction de nettoyage de texte\n",
    "    def _clean_text(self,text):\n",
    "        # Retirer les caractères non-alphabétiques et convertir en minuscules\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        # Retirer les stop words si besoin\n",
    "        if not ALLOW_STOPWORD:\n",
    "            tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
    "        # Ajouter les tokens de début et de fin\n",
    "        tokens.insert(0, START_TOKEN)  # Insérer le token de début en première position\n",
    "        tokens.append(END_TOKEN)  # Ajouter le token de fin\n",
    "        return tokens\n",
    "    \n",
    "    def __init__(self, ensemble):\n",
    "        self.ensemble = ensemble\n",
    "        \n",
    "        # Créer une liste de tous les IDs d'images\n",
    "        self.imgIds = coco_instances.getImgIds()\n",
    "        start, stop = self._getsplit(ensemble)\n",
    "        self.ids = self.imgIds[start:stop]\n",
    "        self.captions_ids = { id : coco_captions.getAnnIds(imgIds=id) for id in self.ids }\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.ids) / BATCH_SIZE))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_ids = self.ids[index * BATCH_SIZE : (index + 1) * BATCH_SIZE]\n",
    "        batch_images = []\n",
    "        batch_captions = []\n",
    "        max_len_captions = 0\n",
    "        for id in batch_ids:\n",
    "            # Charger l'image\n",
    "            file_name = coco_instances.imgs[id]['file_name']\n",
    "            image = Image.open(f'{DATADIR}/{file_name}')\n",
    "            image = image.resize((224, 224))\n",
    "            image = image.convert('RGB')\n",
    "            image = np.array(image)\n",
    "            batch_images.append(image)\n",
    "            # Charger une légende aléatoire\n",
    "            caption_ids = self.captions_ids[id]\n",
    "            chosen_id = np.random.choice(caption_ids)\n",
    "            caption = coco_captions.anns[chosen_id]['caption'] # Accès directe car API buggée\n",
    "            caption = self._clean_text(caption)\n",
    "            caption_vector = [wordvec.wv.key_to_index[word] for word in caption if word in wordvec.wv.key_to_index]\n",
    "            len_caption = len(caption_vector)\n",
    "            if len_caption > max_len_captions:\n",
    "                max_len_captions = len_caption\n",
    "            batch_captions.append(caption_vector)\n",
    "        batch_images = preprocess_input(np.array(batch_images).copy())\n",
    "        batch_captions = pad_sequences(batch_captions, maxlen=max_len_captions, padding='post')\n",
    "        return batch_images, batch_captions\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.ids = np.random.permutation(self.ids)\n",
    "\n",
    "train_generator = DatasetGenerator('train')\n",
    "val_generator = DatasetGenerator('val')\n",
    "test_generator = DatasetGenerator('test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test unitaire du générateur de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = train_generator\n",
    "# Récupérer un batch d'images et de légendes\n",
    "r_index = np.random.randint(len(generator))\n",
    "images, captions = generator.__getitem__(r_index-1)\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Captions shape: {captions.shape}\")\n",
    "# Plot d'une des images avec sa légende\n",
    "r_index = np.random.randint(0, images.shape[0])\n",
    "selected_image = images[r_index]\n",
    "selected_caption = captions[r_index]\n",
    "\n",
    "# On recentre les valeurs de l'image\n",
    "selected_image = ( selected_image - np.min(selected_image) ) / ( np.max(selected_image) - np.min(selected_image) ) * 255\n",
    "# On convertit l'image en RGB pour l'affichage\n",
    "selected_image = np.array(selected_image)\n",
    "selected_image = selected_image.astype('uint8')\n",
    "selected_image = selected_image[...,::-1]\n",
    "\n",
    "# Convertir les indices de la légende en mots\n",
    "selected_caption_words = [wordvec.wv.index_to_key[index] for index in selected_caption if index in wordvec.wv.index_to_key]\n",
    "selected_caption_str = ''.join(selected_caption_words).replace(START_TOKEN, '').replace(END_TOKEN, '')\n",
    "\n",
    "# Affichage de l'image et de la légende\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(selected_image)\n",
    "plt.title(f\"Caption: {selected_caption_str}\")\n",
    "plt.axis('off')  # Désactiver les axes pour une meilleure visibilité\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de la layer d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embedding(wordvec, word):\n",
    "    # On test la layer d'embedding de tf\n",
    "    if word not in wordvec.wv.key_to_index:\n",
    "        print(f\"Le mot '{word}' n'est pas dans le vocabulaire.\")\n",
    "        return\n",
    "    word_index = wordvec.wv.key_to_index[word]\n",
    "    embedding = wordvec.wv.get_vector(word)\n",
    "    print(f\"Index du mot '{word}' dans le vocabulaire : {word_index}\")\n",
    "    print(f\"Embedding du mot '{word} (wordvec)' : {embedding[0:5]}..\")\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=TEXT_VECTOR_SIZE, weights=[wordvec.wv.vectors], trainable=False))\n",
    "    # Test de l'embedding\n",
    "    embedded_word = model.predict(np.array([[word_index]]))\n",
    "    print(f\"Embedding du mot '{word}' (calculé) : {embedded_word[0][0][0:5]}..\")\n",
    "    \n",
    "    print(\"Les deux embeddings sont-ils égaux ? :\", np.allclose(embedding, embedded_word[0][0]))\n",
    "\n",
    "test_embedding(wordvec, input('Quel mot souhaitez-vous tester ? :'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def caption_model():\n",
    "    '''\n",
    "    inputs :\n",
    "        image : (batch_size, 224, 224, 3)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        output : (batch_size, TEXT_VECTOR_SIZE)\n",
    "    '''\n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    image_data = layers.Dense(IMAGE_VECTOR_SIZE)(x)  # Nouvelle couche dense pour les caractéristiques\n",
    "    \n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,TEXT_VECTOR_SIZE))\n",
    "    x = layers.Masking(mask_value=0.0)(text_input) # Extrèmement important\n",
    "    text_data = layers.LSTM(512)(x)\n",
    "    context = layers.Concatenate()([image_data, text_data])\n",
    "    output = layers.Dense(TEXT_VECTOR_SIZE, activation='relu')(context)\n",
    "    \n",
    "    model = Model(inputs=[base_model.input, text_input], outputs=[output], name='caption_model')\n",
    "    cosinus_loss = losses.CosineSimilarity()\n",
    "    model.compile(loss=cosinus_loss, optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = caption_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='caption_model.png', show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
