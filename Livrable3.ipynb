{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models, losses, callbacks\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from pycocotools.coco import COCO\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from tensorflow.keras.models import load_model\n",
    "import sys\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(gpus)\n",
    "print(tf.__version__)\n",
    "\n",
    "print(\"Eager execution:\", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes et Variables et Fonctions Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTDIR = 'annotations_trainval2014'\n",
    "DATADIR = 'train2014'\n",
    "CAPFILE = '{}/annotations/captions_{}.json'.format(ANNOTDIR, DATADIR)\n",
    "INSTANCEFILE = '{}/annotations/instances_{}.json'.format(ANNOTDIR, DATADIR)\n",
    "ALLOW_STOPWORD = True\n",
    "TEXT_VECTOR_SIZE = 512 #VOCAB_SIZE = 24918\n",
    "WORD2VEC_PATH = f'word2vec_captions_{TEXT_VECTOR_SIZE}.txt'\n",
    "MAX_LEN_SEQUENCE = 60 # 57 Obtenu via trainement des données\n",
    "START_TOKEN = '<sos>'\n",
    "END_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "PADDING_TOKEN = '<pad>'\n",
    "coco_captions = COCO(CAPFILE)\n",
    "coco_instances = COCO(INSTANCEFILE)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "RATIO_TRAIN = 0.8\n",
    "RATIO_VAL = 0.15\n",
    "RATIO_TEST = 0.05\n",
    "PATIENCE = 3\n",
    "\n",
    "if os.path.exists(WORD2VEC_PATH):\n",
    "    try :\n",
    "        global VOCAB_SIZE, PADDING_INDEX, vec\n",
    "        vec = KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary=False)\n",
    "        print(f\"KeyedVectors loaded from {WORD2VEC_PATH}\")\n",
    "        PADDING_INDEX = vec.add_vector(PADDING_TOKEN, np.zeros(TEXT_VECTOR_SIZE))\n",
    "        print(f\"Padding index : {PADDING_INDEX}\")\n",
    "        VOCAB_SIZE = len(vec.index_to_key)\n",
    "        print(\"VOCAB_SIZE :\",VOCAB_SIZE)\n",
    "        \n",
    "    except e as Exception:\n",
    "        print(f\"Error loading KeyedVectors from {WORD2VEC_PATH} error : {e}\")\n",
    "        vec = None\n",
    "else:\n",
    "    print(f\"No model found at {WORD2VEC_PATH}\")\n",
    "\n",
    "assert RATIO_TRAIN + RATIO_VAL + RATIO_TEST == 1 # Vérification de la somme des ratios\n",
    "assert TEXT_VECTOR_SIZE == vec.vector_size # Vérification de la taille des vecteurs\n",
    "\n",
    "def find_closest_word(vector,):\n",
    "    # Cas pour le padding\n",
    "    if np.all(vector == 0):\n",
    "        return PADDING_TOKEN\n",
    "    # Calculer la similarité de cosine entre le vecteur donné et tous les vecteurs dans Word2Vec\n",
    "    similarities = cosine_similarity([vector], vec.vectors)[0] # 0 -> tuple (word, similarity) 0 -> word  PS : Fonction O(n) mais très optimisé (493 μs ± 4.53)\n",
    "    # Trouver l'index du vecteur le plus similaire\n",
    "    closest_index = similarities.argmax()\n",
    "    # Retourner le mot correspondant à cet index\n",
    "    return vec.index_to_key[closest_index]\n",
    "\n",
    "def find_closest_vector(vector, vec, printable=False):\n",
    "    # Calculer la similarité de cosine entre le vecteur donné et tous les vecteurs dans Word2Vec\n",
    "    similarities = cosine_similarity([vector], vec.vectors)[0]\n",
    "    if printable:\n",
    "        print(f'Similarities : {similarities}')\n",
    "    # Récupérer le vecteur le plus similaire\n",
    "    closest_vector = vec.vectors[similarities.argmax()]\n",
    "    return closest_vector\n",
    "\n",
    "def get_positional_encoding(max_seq_length, embed_size, scale=10000):\n",
    "    positional_encoding = np.array([\n",
    "        [pos / np.power(scale, 2 * (i//2) / embed_size) for i in range(embed_size)]\n",
    "        if pos != 0 else np.zeros(embed_size) for pos in range(max_seq_length)\n",
    "    ], dtype=np.float32)\n",
    "    positional_encoding[:, 0::2] = np.sin(positional_encoding[:, 0::2])  # dimensions 2i\n",
    "    positional_encoding[:, 1::2] = np.cos(positional_encoding[:, 1::2])  # dimensions 2i+1\n",
    "    return tf.cast(positional_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEncodingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_seq_length, embed_size, scale=10000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embed_size = embed_size\n",
    "        self.positional_encoding = get_positional_encoding(max_seq_length, embed_size, scale=scale)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        seq_length = tf.shape(x)[1]\n",
    "        # Réduit positional_encoding à la longueur de la séquence réelle en cas de séquence plus courte que max_seq_length\n",
    "        pe = self.positional_encoding[:seq_length, :]\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Étend le masque pour qu'il ait la même dimension que x et pe\n",
    "            mask = tf.cast(mask, tf.float32)\n",
    "            mask = tf.expand_dims(mask, axis=-1)\n",
    "            # Utilise le masque pour annuler l'encoding sur les positions masquées\n",
    "            pe *= mask\n",
    "        \n",
    "        return x + pe\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"max_seq_length\": self.max_seq_length,\n",
    "            \"embed_size\": self.embed_size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class PaddingTruncatingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen=MAX_LEN_SEQUENCE, padding_value=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.maxlen = maxlen\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Obtient la taille réelle des séquences\n",
    "        input_shape = tf.shape(inputs)\n",
    "\n",
    "        # Tronque les séquences si elles sont plus longues que maxlen\n",
    "        inputs = inputs[:, input_shape[1]-self.maxlen:, :]\n",
    "\n",
    "        # Calcule le padding nécessaire\n",
    "        padding_size = self.maxlen - tf.shape(inputs)[1]\n",
    "\n",
    "        # Crée un padding de taille [batch_size, padding_size, features_dim]\n",
    "        padding = tf.fill([input_shape[0], padding_size, input_shape[2]], self.padding_value)\n",
    "        padding = tf.cast(padding, tf.float32)\n",
    "\n",
    "        # Concatène le padding à l'input pour atteindre maxlen\n",
    "        out_tensor = tf.concat([inputs, padding], axis=1)\n",
    "        return tf.reshape(out_tensor, self.compute_output_shape(input_shape))\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.maxlen, input_shape[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sélectionner un ID d'image au hasard\n",
    "imgIds = coco_instances.getImgIds()\n",
    "print(f' Number of images found in instances :',len(imgIds))\n",
    "randomImgId = np.random.choice(imgIds)\n",
    "found_img = coco_instances.imgs[randomImgId]\n",
    "file_name = found_img['file_name']\n",
    "print(f\" Filename : {file_name}\")\n",
    "\n",
    "image = Image.open(f'{DATADIR}/{file_name}')\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Désactiver les axes, qui ne sont pas nécessaires pour l'affichage d'image\n",
    "plt.show()\n",
    "\n",
    "# Récupérer les IDs des annotations de légendes pour l'image sélectionnée\n",
    "annIds = coco_captions.getAnnIds(imgIds=randomImgId)\n",
    "# Charger les annotations\n",
    "anns = coco_captions.loadAnns(annIds)\n",
    "# Afficher les légendes\n",
    "print(\"Captions for the selected image:\")\n",
    "for ann in anns:\n",
    "    print(f\"- {ann['caption']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-Traitement pour Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de nettoyage de texte\n",
    "def process_text(text):\n",
    "    # Retirer les caractères non-alphabétiques et convertir en minuscules\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Retirer les stop words si besoin\n",
    "    if not ALLOW_STOPWORD:\n",
    "        tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
    "    # Ajouter les tokens de début et de fin\n",
    "    tokens.insert(0, START_TOKEN)  # Insérer le token de début en première position\n",
    "    tokens.append(END_TOKEN)  # Ajouter le token de fin\n",
    "    return tokens\n",
    "\n",
    "count_captions = 0\n",
    "count_invidual_captions = 0\n",
    "raw_captions = []\n",
    "for id in imgIds :\n",
    "    caption_ids = coco_captions.getAnnIds(imgIds=id)\n",
    "    captions_data = coco_captions.loadAnns(caption_ids)\n",
    "    captions = [process_text(caption['caption']) for caption in captions_data]\n",
    "    count_invidual_captions += len(captions)\n",
    "    count_captions += 1\n",
    "    raw_captions += captions # On aura donc raw_captions une liste de listes\n",
    "max_captions = max([len(raw_captions[i]) for i in range(len(raw_captions))])\n",
    "max_len_captions = max([len(raw_captions[i][j]) for i in range(len(raw_captions)) for j in range(len(raw_captions[i]))])\n",
    "print('Attention, statistique avec captions altérés (ajout des tokens de début et de fin)')\n",
    "print('count_captions :',count_captions)\n",
    "print('count_invidual_captions :',count_invidual_captions)\n",
    "print('mean number of caption per image :',count_invidual_captions/count_captions)\n",
    "print('max number of captions :',max_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner un modèle Word2Vec\n",
    "wordvec = Word2Vec(raw_captions, vector_size=TEXT_VECTOR_SIZE, window=4, min_count=1, workers=3, epochs=100)\n",
    "\n",
    "# Nombre de mots dans le vocabulaire\n",
    "vocab_size = len(wordvec.wv.key_to_index)\n",
    "print(f\"Nombre de mots dans le vocabulaire : {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sauvegarde de Word2Vec (données uniquement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec.wv.save_word2vec_format(WORD2VEC_PATH, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec = Word2Vec.load(WORD2VEC_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test unitaire de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = input('Quel mot souhaitez-vous avoir de similaire ? :')\n",
    "#START_TOKEN = '<sos>'\n",
    "#END_TOKEN = '<eos>'\n",
    "\n",
    "if word in vec.key_to_index:\n",
    "    similar_words = vec.most_similar(word)\n",
    "    print(\"Mots similaires à '{}':\".format(word))\n",
    "    for similar_word, similarity in similar_words:\n",
    "        print(f\"{similar_word}: {similarity:.4f}\")\n",
    "else:\n",
    "    # Si le mot n'est pas dans le vocabulaire, afficher un message d'erreur\n",
    "    print(\"Désolé, le mot '{}' n'est pas dans le vocabulaire.\".format(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Générateur de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des générateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator(Sequence):\n",
    "    def _getsplit(self, ensemble):\n",
    "        if ensemble == 'train':\n",
    "            start = 0\n",
    "            stop = int(RATIO_TRAIN * len(self.imgIds))\n",
    "        elif ensemble == 'val':\n",
    "            start = int(RATIO_TRAIN * len(self.imgIds))\n",
    "            stop = int((RATIO_TRAIN + RATIO_VAL) * len(self.imgIds))\n",
    "        elif ensemble == 'test':\n",
    "            start = int((RATIO_TRAIN + RATIO_VAL) * len(self.imgIds))\n",
    "            stop = len(self.imgIds)\n",
    "        return start, stop\n",
    "    \n",
    "    # Fonction de nettoyage de texte\n",
    "    def _clean_text(self,text):\n",
    "        # Retirer les caractères non-alphabétiques et convertir en minuscules\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        # Retirer les stop words si besoin\n",
    "        if not ALLOW_STOPWORD:\n",
    "            tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
    "        # Ajouter les tokens de début et de fin\n",
    "        tokens.insert(0, START_TOKEN)  # Insérer le token de début en première position\n",
    "        tokens.append(END_TOKEN)  # Ajouter le token de fin\n",
    "        return tokens\n",
    "    \n",
    "    def __init__(self, ensemble, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ensemble = ensemble\n",
    "        \n",
    "        # Créer une liste de tous les IDs d'images\n",
    "        self.imgIds = coco_instances.getImgIds()\n",
    "        start, stop = self._getsplit(ensemble)\n",
    "        self.ids = self.imgIds[start:stop]\n",
    "        self.captions_ids = { id : coco_captions.getAnnIds(imgIds=id) for id in self.ids }\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.ids) / BATCH_SIZE))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_ids = self.ids[index * BATCH_SIZE : (index + 1) * BATCH_SIZE]\n",
    "        batch_images = []\n",
    "        batch_captions = []\n",
    "        batch_labels = []\n",
    "        max_len_captions = 0\n",
    "        for id in batch_ids:\n",
    "            # Charger l'image\n",
    "            file_name = coco_instances.imgs[id]['file_name']\n",
    "            image = Image.open(f'{DATADIR}/{file_name}')\n",
    "            image = image.resize((224, 224))\n",
    "            image = image.convert('RGB')\n",
    "            image = np.array(image)\n",
    "            batch_images.append(image)\n",
    "            # Charger une légende aléatoire\n",
    "            caption_ids = self.captions_ids[id]\n",
    "            chosen_id = np.random.choice(caption_ids)\n",
    "            caption = coco_captions.anns[chosen_id]['caption'] # Accès directe car API buggée\n",
    "            caption = self._clean_text(caption)\n",
    "            r_index = np.random.randint(1, len(caption)) # On ne prend pas le token de début\n",
    "            caption_crop = caption[:r_index] # On crop la légende pour l'entrainement du modèle\n",
    "            caption_label = caption[r_index] # On garde le mot suivant pour la prédiction\n",
    "            caption_indexs = [ vec.key_to_index[caption_crop[i]] for i in range(len(caption_crop))]\n",
    "            caption_index_label = vec.key_to_index[caption_label]\n",
    "            len_caption = len(caption_indexs)\n",
    "            if len_caption > max_len_captions:\n",
    "                max_len_captions = len_caption\n",
    "            batch_captions.append(caption_indexs)\n",
    "            batch_labels.append(caption_index_label)\n",
    "        batch_images = preprocess_input(np.array(batch_images).copy())\n",
    "        batch_captions = pad_sequences(batch_captions, maxlen=max_len_captions, padding='post', value=PADDING_INDEX, dtype='float32')\n",
    "        batch_labels = np.array(batch_labels)\n",
    "\n",
    "        return ((batch_images, batch_captions), batch_labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.ids = np.random.permutation(self.ids)\n",
    "\n",
    "train_generator = DatasetGenerator('train')\n",
    "val_generator = DatasetGenerator('val')\n",
    "test_generator = DatasetGenerator('test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test unitaire du générateur de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = train_generator\n",
    "# Récupérer un batch d'images et de légendes\n",
    "r_index = np.random.randint(len(generator))\n",
    "x, labels = generator.__getitem__(r_index-1)\n",
    "images, captions = x\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Captions shape: {captions.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "# Plot d'une des images avec sa légende\n",
    "r_index = np.random.randint(0, images.shape[0])\n",
    "selected_image = images[r_index]\n",
    "selected_caption = captions[r_index]\n",
    "selected_caption = np.array(selected_caption, dtype='int16')\n",
    "selected_label = vec.index_to_key[labels[r_index]]\n",
    "\n",
    "# On recentre les valeurs de l'image\n",
    "selected_image = ( selected_image - np.min(selected_image) ) / ( np.max(selected_image) - np.min(selected_image) ) * 255\n",
    "# On convertit l'image en RGB pour l'affichage\n",
    "selected_image = np.array(selected_image)\n",
    "selected_image = selected_image.astype('uint8')\n",
    "selected_image = selected_image[...,::-1]\n",
    "\n",
    "# Convertir les indices de la légende en mots\n",
    "selected_caption_words = [ vec.index_to_key[index] for index in selected_caption]\n",
    "selected_caption_str = ' '.join(selected_caption_words)\n",
    "# Affichage de l'image et de la légende\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(selected_image)\n",
    "plt.title(f\"(Input: {selected_caption_str}) (Label: {selected_label})\")\n",
    "plt.axis('off')  # Désactiver les axes pour une meilleure visibilité\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de la layer d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embedding(word):\n",
    "    # On test la layer d'embedding de tf\n",
    "    if word not in vec.key_to_index:\n",
    "        print(f\"Le mot '{word}' n'est pas dans le vocabulaire.\")\n",
    "        return\n",
    "    word_index = vec.key_to_index[word]\n",
    "    embedding = vec.get_vector(word)\n",
    "    print(f\"Index du mot '{word}' dans le vocabulaire : {word_index}\")\n",
    "    print(f\"Embedding du mot '{word} (wordvec)' : {embedding[0:5]}..\")\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(1,)))\n",
    "    model.add(layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False))\n",
    "    model.summary()\n",
    "    # Test de l'embedding\n",
    "    embedded_word = model.predict(np.array([[word_index]]))\n",
    "    print(f\"Embedding du mot '{word}' (calculé) : {embedded_word[0][0][0:5]}..\")\n",
    "    \n",
    "    print(\"Les deux embeddings sont-ils égaux ? :\", np.allclose(embedding, embedded_word[0][0]))\n",
    "\n",
    "test_embedding(input('Quel mot souhaitez-vous tester ? :'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de la layer de positionnal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_pe(length, scale):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(None,)))\n",
    "    model.add(layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False))\n",
    "    model.add(PositionalEncodingLayer(max_seq_length=MAX_LEN_SEQUENCE, embed_size=TEXT_VECTOR_SIZE, scale=scale))\n",
    "    # Test de l'encoding avec un batch de données\n",
    "    r_index = np.random.randint(len(train_generator))\n",
    "    x, y = train_generator.__getitem__(r_index)\n",
    "    x0, x1 = x\n",
    "    try :\n",
    "        model.predict(x1, verbose=0)\n",
    "        print(\"PE fonctionne correctement sur un batch de données.\")\n",
    "    except e as Exception:\n",
    "        print(f\"Erreur lors de l'application de PE sur un batch de données : {e}\")\n",
    "    \n",
    "    # Test de l'encoding avec un masque de zéros\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(None,TEXT_VECTOR_SIZE)))\n",
    "    model.add(PositionalEncodingLayer(max_seq_length=MAX_LEN_SEQUENCE, embed_size=TEXT_VECTOR_SIZE, scale=scale))\n",
    "    try :\n",
    "        zero_mask = np.zeros((BATCH_SIZE, length, TEXT_VECTOR_SIZE), dtype='float32')\n",
    "        pe = model.predict(zero_mask, verbose=0)\n",
    "        print(\"PE fonctionne correctement avec un masque de zéros.\")\n",
    "    except e as Exception:\n",
    "        print(f\"Erreur lors de l'application de PE avec un masque de zéros : {e}\")\n",
    "    \n",
    "    if pe is not None:\n",
    "        # Affichage de l'encoding\n",
    "        plt.figure(figsize=(25, 5))\n",
    "        pe0 = pe[0]\n",
    "        # Plot des valeurs pair et impair\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        ax[0].plot(pe0[:, 0::2])\n",
    "        ax[0].set_title('Dimensions paires')\n",
    "        ax[1].plot(pe0[:, 1::2])\n",
    "        ax[1].set_title('Dimensions impaires')\n",
    "        plt.show()\n",
    "\n",
    "test_pe(length=60, scale=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de la layer de Padding Truncage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_padding():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(None,)))\n",
    "    model.add(layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False))\n",
    "    model.add(PaddingTruncatingLayer(maxlen=MAX_LEN_SEQUENCE))\n",
    "    model.summary()\n",
    "    # Test de la couche de padding avec un batch de données\n",
    "    r_index = np.random.randint(len(train_generator))\n",
    "    x, y = train_generator.__getitem__(r_index)\n",
    "    x0, x1 = x\n",
    "    # Choix aléatoire d'un index dans le batch\n",
    "    r_index = np.random.randint(x1.shape[0])\n",
    "    selected_caption = x1[r_index]\n",
    "    selected_image = x0[r_index]\n",
    "    # On recentre les valeurs de l'image\n",
    "    selected_image = ( selected_image - np.min(selected_image) ) / ( np.max(selected_image) - np.min(selected_image) ) * 255\n",
    "    # On convertit l'image en RGB pour l'affichage\n",
    "    selected_image = np.array(selected_image)\n",
    "    selected_image = selected_image.astype('uint8')\n",
    "    selected_image = selected_image[...,::-1]\n",
    "    selected_caption = np.expand_dims(selected_caption, axis=0)\n",
    "    res = None\n",
    "    res = model.predict(selected_caption, verbose=0)[0] # x1 : batch de captions  selected_caption : caption sélectionnée\n",
    "    true_caption = [ vec.index_to_key[int(selected_caption[0][i])] for i in range(len(selected_caption[0])) ]\n",
    "    true_caption_str = ' '.join(true_caption)\n",
    "    print(f'True caption : {true_caption_str}')\n",
    "    # On cherche les mot donc les vecteurs sont similaires depuis \"res\"\n",
    "    selected_caption_words = [ find_closest_word(res[i]) for i in range(len(res))]\n",
    "    selected_caption_words_str = ' '.join(selected_caption_words)\n",
    "    print(f'Model caption : {selected_caption_words_str}')\n",
    "    print(f\"Lenght of true caption : {len(true_caption)}\")\n",
    "    print(f\"Lenght of Model caption : {len(selected_caption_words)}\")\n",
    "    # On plot le résultat du padding\n",
    "    plt.figure(figsize=(25, 5))\n",
    "    plt.imshow(selected_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "test_padding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de la layer d'attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attention(shape1, shape2):\n",
    "    input1 = layers.Input(shape=shape1)\n",
    "    input2 = layers.Input(shape=shape2)\n",
    "    attention = layers.Attention()([input1, input2])\n",
    "    model = Model(inputs=[input1, input2], outputs=attention)\n",
    "    model.summary()\n",
    "\n",
    "test_attention((64, 256), (63, 196))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de la layer Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multi_head_attention(shape1, shape2, key_dim=128, num_heads=1):\n",
    "    input1 = layers.Input(shape=shape1)\n",
    "    input2 = layers.Input(shape=shape2)\n",
    "    # Création de la couche d'attention\n",
    "    attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(input1, input2)\n",
    "    model = Model(inputs=[input1, input2], outputs=attention)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "test_multi_head_attention((60,256),(49,256),1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def caption_modelv1():\n",
    "    '''\n",
    "    inputs :\n",
    "        image : (batch_size, 224, 224, 3)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        output : (batch_size, TEXT_VECTOR_SIZE)\n",
    "    results :\n",
    "        cosine similarity loss : -0.48\n",
    "    '''\n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    image_data = layers.Dense(256)(x)  # Nouvelle couche dense pour les caractéristiques\n",
    "    \n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,TEXT_VECTOR_SIZE))\n",
    "    x = layers.Masking(mask_value=0.0)(text_input) # Extrèmement important\n",
    "    text_data = layers.LSTM(512)(x)\n",
    "    context = layers.Concatenate()([image_data, text_data])\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Dense(2024)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    output = layers.Dense(TEXT_VECTOR_SIZE, activation='relu')(context)\n",
    "    \n",
    "    model = Model(inputs=(base_model.input, text_input), outputs=[output], name='caption_modelv1')\n",
    "    cosinus_loss = losses.CosineSimilarity()\n",
    "    model.compile(loss=cosinus_loss, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def caption_modelv2():\n",
    "    '''\n",
    "    inputs :\n",
    "        image : (batch_size, 224, 224, 3)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        output : (batch_size, TEXT_VECTOR_SIZE)\n",
    "    results :\n",
    "        sparse_categorical_crossentropy : 2.6\n",
    "        sparse_categorical_crossentropy : 2.9   # Version lourde\n",
    "        sparse_categorical_crossentropy : 2.55 # V2\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False # True pour fine-tuning ou si beaucoup de mémoire disponible\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    image_data = layers.Activation('leaky_relu')(x)\n",
    "\n",
    "    \n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,))\n",
    "    x = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False)(text_input)\n",
    "    x = layers.Masking(mask_value=0.0)(x) # Extrèmement important    \n",
    "    x = layers.LSTM(1024, return_sequences=False)(x) # return_sequences = True pour obtenir une sortie pour chaque mot\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    text_data = layers.Activation('leaky_relu')(x)\n",
    "    context = layers.Concatenate()([image_data, text_data])\n",
    "    context = layers.Dropout(0.1)(context)\n",
    "    output = layers.Dense(VOCAB_SIZE, activation='softmax')(context)\n",
    "    \n",
    "    model = Model(inputs=(base_model.input, text_input), outputs=[output], name='caption_modelv2')\n",
    "    loss = losses.sparse_categorical_crossentropy\n",
    "    model.compile(loss=loss, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def caption_modelv3():\n",
    "    '''\n",
    "    inputs :\n",
    "        image : (batch_size, 224, 224, 3)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        output : (batch_size, TEXT_VECTOR_SIZE)\n",
    "    results :\n",
    "        sparse_categorical_crossentropy : 5.2137\n",
    "    '''\n",
    "    \n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False # True pour fine-tuning ou si beaucoup de mémoire disponible\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    image_data = layers.Dense(256)(x)  # Nouvelle couche dense pour les caractéristiques\n",
    "    image_data_expanded = tf.expand_dims(image_data, 1)  # Ajoute une dimension de séquence\n",
    "    \n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,))\n",
    "    image_data_tiled = tf.tile(image_data_expanded, [1, tf.shape(text_input)[1], 1])  # Réplique le long de la dimension de séquence\n",
    "    text_features  = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False)(text_input)\n",
    "    text_features  = layers.Masking(mask_value=0.0)(text_features) # Extrèmement important\n",
    "    combined_features = layers.Concatenate(axis=-1)([text_features, image_data_tiled])\n",
    "    context = layers.Bidirectional(layers.LSTM(1024))(combined_features) # return_sequences = True pour obtenir une sortie pour chaque mot\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    context = layers.Dense(256)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    output = layers.Dense(VOCAB_SIZE, activation='softmax')(context)\n",
    "    \n",
    "    model = Model(inputs=(base_model.input, text_input), outputs=[output], name='caption_modelv3')\n",
    "    loss = losses.sparse_categorical_crossentropy\n",
    "    model.compile(loss=loss, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def caption_modelv4():\n",
    "    '''\n",
    "    inputs :\n",
    "        image : (batch_size, 224, 224, 3)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        output : (batch_size, TEXT_VECTOR_SIZE)\n",
    "    results :\n",
    "        sparse_categorical_crossentropy : 2.98     # self-attention avec causal_mask=False sans PE\n",
    "        sparse_categorical_crossentropy : 2.88     # self-attention avec causal_mask=True sans PE\n",
    "        sparse_categorical_crossentropy : 3.17     # self-attention avec causal_mask=True avec PE\n",
    "        sparse_categorical_crossentropy : 3.47        # self-attention avec causal_mask=True sans PE et avec modèle lourd (head=64 LSTM 1024-512-256 + 2x Dense 256)\n",
    "    '''\n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False # True pour fine-tuning ou si beaucoup de mémoire disponible\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    image_data = layers.Dense(256)(x)  # Nouvelle couche dense pour les caractéristiques\n",
    "\n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,))\n",
    "    text_features  = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False)(text_input)\n",
    "    #text_features = PositionalEncodingLayer(max_seq_length=MAX_LEN_SEQUENCE, embed_size=TEXT_VECTOR_SIZE, scale=100)(text_features)\n",
    "    text_features  = layers.Masking(mask_value=0.0)(text_features) # Extrèmement important\n",
    "    \n",
    "    # Self-Attention mechanism\n",
    "    self_attention_text = layers.MultiHeadAttention(num_heads=64, key_dim=TEXT_VECTOR_SIZE)(text_features, text_features)\n",
    "    self_attention_text.use_causal_mask = True\n",
    "    self_attention_text = layers.Add()([text_features, self_attention_text])\n",
    "    self_attention_text = layers.LayerNormalization()(self_attention_text)\n",
    "    self_attention_text = layers.Activation('leaky_relu')(self_attention_text)\n",
    "\n",
    "    text_data = layers.Bidirectional(layers.LSTM(1024, return_sequences=True))(self_attention_text)\n",
    "    text_data = layers.BatchNormalization()(text_data)\n",
    "    text_data = layers.Activation('leaky_relu')(text_data)\n",
    "    text_data = layers.Bidirectional(layers.LSTM(512, return_sequences=True))(text_data)\n",
    "    text_data = layers.BatchNormalization()(text_data)\n",
    "    text_data = layers.Activation('leaky_relu')(text_data)\n",
    "    text_data = layers.Bidirectional(layers.LSTM(256))(text_data)\n",
    "    text_data = layers.BatchNormalization()(text_data)\n",
    "    text_data = layers.Activation('leaky_relu')(text_data)\n",
    "    context = layers.Concatenate()([image_data, text_data])\n",
    "    context = layers.Dense(256)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    context = layers.Dense(256)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    output = layers.Dense(VOCAB_SIZE, activation='softmax')(context)\n",
    "\n",
    "    model = Model(inputs=(base_model.input, text_input), outputs=[output], name='caption_modelv4')\n",
    "    loss = losses.sparse_categorical_crossentropy\n",
    "    model.compile(loss=loss, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def caption_modelv5():  \n",
    "    '''\n",
    "    inputs :\n",
    "        image : (batch_size, 224, 224, 3)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        output : (batch_size, TEXT_VECTOR_SIZE)\n",
    "    results :\n",
    "        sparse_categorical_crossentropy : 4.16\n",
    "    '''\n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False # True pour fine-tuning ou si beaucoup de mémoire disponible\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    image_data = layers.Dense(256)(x)  # Nouvelle couche dense pour les caractéristiques\n",
    "\n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,))\n",
    "    text_features  = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False)(text_input)\n",
    "    #text_features = PositionalEncodingLayer(max_seq_length=MAX_LEN_SEQUENCE, embed_size=TEXT_VECTOR_SIZE, scale=100)(text_features)\n",
    "    text_features  = layers.Masking(mask_value=0.0)(text_features) # Extrèmement important\n",
    "    \n",
    "    # Self-Attention mechanism\n",
    "    self_attention_text = layers.MultiHeadAttention(num_heads=64, key_dim=TEXT_VECTOR_SIZE)(text_features, text_features)\n",
    "    self_attention_text.use_causal_mask = True\n",
    "    self_attention_text = layers.Add()([text_features, self_attention_text])\n",
    "    self_attention_text = layers.LayerNormalization()(self_attention_text)\n",
    "    self_attention_text = layers.Activation('leaky_relu')(self_attention_text)\n",
    "\n",
    "    # Prepare image data for cross-attention by repeating it to match text sequence length\n",
    "    seq_length = tf.shape(text_features)[1]  # Get the sequence length of text features\n",
    "    image_data_expanded = tf.expand_dims(image_data, 1)  # Expand dims to simulate sequence length\n",
    "    image_features_for_attention = tf.tile(image_data_expanded, [1, seq_length, 1])  # Tile across the sequence length\n",
    "\n",
    "    # Cross-Attention mechanism\n",
    "    cross_attention_text = layers.MultiHeadAttention(num_heads=64, key_dim=TEXT_VECTOR_SIZE)(self_attention_text, image_features_for_attention)\n",
    "    cross_attention_text = layers.Add()([self_attention_text, cross_attention_text])\n",
    "    cross_attention_text = layers.LayerNormalization()(cross_attention_text)\n",
    "    cross_attention_text = layers.Activation('leaky_relu')(cross_attention_text)\n",
    "\n",
    "    text_data = layers.Bidirectional(layers.LSTM(1024, return_sequences=True))(self_attention_text)\n",
    "    text_data = layers.BatchNormalization()(text_data)\n",
    "    text_data = layers.Activation('leaky_relu')(text_data)\n",
    "    text_data = layers.Bidirectional(layers.LSTM(512, return_sequences=True))(text_data)\n",
    "    text_data = layers.BatchNormalization()(text_data)\n",
    "    text_data = layers.Activation('leaky_relu')(text_data)\n",
    "    text_data = layers.Bidirectional(layers.LSTM(256))(text_data)\n",
    "    text_data = layers.BatchNormalization()(text_data)\n",
    "    text_data = layers.Activation('leaky_relu')(text_data)\n",
    "    context = layers.Concatenate()([image_data, text_data])\n",
    "    context = layers.Dense(256)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    output = layers.Dense(VOCAB_SIZE, activation='softmax')(context)\n",
    "\n",
    "    model = Model(inputs=(base_model.input, text_input), outputs=[output], name='caption_modelv5')\n",
    "    loss = losses.sparse_categorical_crossentropy\n",
    "    model.compile(loss=loss, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def caption_modelv6():\n",
    "    def text_block(x, n_head, d_lstm, d_model=TEXT_VECTOR_SIZE, return_sequences=True):\n",
    "        x = layers.MultiHeadAttention(num_heads=n_head, key_dim=d_model)(x, x)  # Self-attention\n",
    "        x.use_causal_mask = True # Mask des tokens futurs\n",
    "        x = layers.Add()([x, x])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = layers.Activation('leaky_relu')(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(d_lstm, return_sequences=return_sequences))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('leaky_relu')(x)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    inputs :\n",
    "        image : (batch_size, 224, 224, 3)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        output : (batch_size, TEXT_VECTOR_SIZE)\n",
    "    results :\n",
    "        sparse_categorical_crossentropy : 3.6\n",
    "\n",
    "    '''\n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False # True pour fine-tuning ou si beaucoup de mémoire disponible\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    image_data = layers.Dense(256)(x)  # Nouvelle couche dense pour les caractéristiques\n",
    "\n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,))\n",
    "    text_features  = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False)(text_input)\n",
    "    #text_features = PositionalEncodingLayer(max_seq_length=MAX_LEN_SEQUENCE, embed_size=TEXT_VECTOR_SIZE, scale=100)(text_features)\n",
    "    text_features  = layers.Masking(mask_value=0.0)(text_features) # Extrèmement important\n",
    "\n",
    "    text_data = text_block(text_features, 64, 1024)\n",
    "    text_data = text_block(text_data, 32, 512)\n",
    "    text_data = text_block(text_data, 16, 256, return_sequences=False)\n",
    "\n",
    "    context = layers.Concatenate()([image_data, text_data])\n",
    "    context = layers.Dense(256)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    context = layers.Dense(256)(context)\n",
    "    context = layers.BatchNormalization()(context)\n",
    "    context = layers.Activation('leaky_relu')(context)\n",
    "    output = layers.Dense(VOCAB_SIZE, activation='softmax')(context)\n",
    "\n",
    "    model = Model(inputs=(base_model.input, text_input), outputs=[output], name='caption_modelv6')\n",
    "    loss = losses.sparse_categorical_crossentropy\n",
    "    model.compile(loss=loss, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def caption_modelv7():\n",
    "    def transformer_decoder_layer(query, key_value, key_dim, num_heads, dff, dropout=0.1,use_causal_mask=False):\n",
    "        \"\"\"\n",
    "        inputs :\n",
    "            query : (batch_size, query_seq_len, dim)\n",
    "            key_value : (batch_size, key_value_seq_len, dim)\n",
    "            key_dim : dimension des clés et valeurs dans la couche d'attention\n",
    "            num_heads : nombre de têtes dans la couche d'attention\n",
    "            dff : multiplieur pour la dimension des couches cachées dans le feed-forward network\n",
    "            dropout : taux de dropout\n",
    "            use_causal_mask : booléen pour utiliser un masque causal dans la couche d'attention\n",
    "        outputs :\n",
    "        \"\"\"\n",
    "        # Multi-Head Attention (utilise causal mask pour respecter l'ordre des mots dans le texte généré)\n",
    "        attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(query, key_value)\n",
    "        attention.use_causal_mask = use_causal_mask\n",
    "        attention = layers.Dropout(dropout)(attention)\n",
    "        attention = layers.Add()([attention, query])\n",
    "        attention = layers.LayerNormalization(epsilon=1e-6)(attention)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        ffn_output = layers.Dense(dff*key_dim, activation='leaky_relu')(attention)\n",
    "        ffn_output = layers.Dense(key_dim)(ffn_output)\n",
    "        ffn_output = layers.Dropout(dropout)(ffn_output)\n",
    "        ffn_output = layers.Add()([ffn_output, attention])\n",
    "        ffn_output = layers.LayerNormalization(epsilon=1e-6)(ffn_output)\n",
    "        return ffn_output\n",
    "    \n",
    "    # Image processing\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    image_input = base_model.input\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False # True pour fine-tuning ou si beaucoup de mémoire disponible\n",
    "    x = base_model.output\n",
    "    x = layers.Reshape((7*7, 2048))(x)\n",
    "    IMAGE_VECTOR_SIZE = TEXT_VECTOR_SIZE # Pour la couche Dense\n",
    "    image_data = layers.Dense(IMAGE_VECTOR_SIZE)(x)\n",
    "    image_data = layers.BatchNormalization()(image_data)\n",
    "    image_data = layers.Activation('leaky_relu')(image_data)\n",
    "    image_data = transformer_decoder_layer(image_data, image_data, IMAGE_VECTOR_SIZE, num_heads=4, dff=4, use_causal_mask=False)\n",
    "    image_data = transformer_decoder_layer(image_data, image_data, IMAGE_VECTOR_SIZE, num_heads=4, dff=4, use_causal_mask=False)\n",
    "\n",
    "\n",
    "    # Text processing\n",
    "    text_input = layers.Input(shape=(None,))\n",
    "    text_features = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=TEXT_VECTOR_SIZE, weights=[vec.vectors], trainable=False)(text_input)\n",
    "    text_features = PaddingTruncatingLayer()(text_features)\n",
    "    text_features = PositionalEncodingLayer(max_seq_length=60, embed_size=TEXT_VECTOR_SIZE, scale=100)(text_features)\n",
    "\n",
    "    # Transformer Decoder\n",
    "    global_dff = 4\n",
    "    global_num_heads = 4\n",
    "\n",
    "    x = transformer_decoder_layer(text_features, text_features, TEXT_VECTOR_SIZE, num_heads=global_num_heads, dff=global_dff, use_causal_mask=True)\n",
    "    x = transformer_decoder_layer(x,x, TEXT_VECTOR_SIZE, num_heads=global_num_heads, dff=global_dff, use_causal_mask=True)\n",
    "    x = transformer_decoder_layer(x,x, TEXT_VECTOR_SIZE, num_heads=global_num_heads, dff=global_dff, use_causal_mask=True)\n",
    "    x = transformer_decoder_layer(x,x, TEXT_VECTOR_SIZE, num_heads=global_num_heads, dff=global_dff, use_causal_mask=True)\n",
    "\n",
    "    x = transformer_decoder_layer(x, image_data, TEXT_VECTOR_SIZE, num_heads=global_num_heads, dff=global_dff)\n",
    "\n",
    "\n",
    "    decoder_output = layers.Lambda(lambda x: x[:, -1, :])(x)\n",
    "\n",
    "    # Final output layer\n",
    "    output = layers.Dense(VOCAB_SIZE, activation='softmax')(decoder_output)\n",
    "\n",
    "    model = Model(inputs=(image_input, text_input), outputs=[output], name='caption_modelv7')\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = caption_modelv7()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement d'un modèle pré-existant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('Livrable3_caption_modelv2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=f'Livrable3_{model.name}.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=PATIENCE, \n",
    "                                         restore_best_weights=True)\n",
    "\n",
    "checkpoint_path = f'checkpoints/{model.name}'\n",
    "checkpoint_path = checkpoint_path + '-{epoch:04d}.keras'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data=val_generator,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'Livrable3_{model.name}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model :\n",
    "    model = load_model('Livrable3_caption_modelv2.keras')\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compilation du modèle\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(loss=losses.sparse_categorical_crossentropy, optimizer=adam) # Attention à choisir la bonne loss\n",
    "\n",
    "# Fine-tuning\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=PATIENCE, \n",
    "                                         restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data=val_generator,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test unitaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_model_inferencev1(model,vec, image):\n",
    "    \"\"\"\n",
    "    inputs :\n",
    "        model : model keras\n",
    "        word2vec : model word2vec\n",
    "        image : (batch_size, 224,224,3) (RGB) (0,255)\n",
    "        text : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        caption : (batch_size, 1, TEXT_VECTOR_SIZE)\n",
    "    \"\"\"\n",
    "    start_index = vec.key_to_index[START_TOKEN]\n",
    "    start_vector = vec.vectors[start_index]\n",
    "    processed_image = preprocess_input(image.copy())\n",
    "    caption = np.expand_dims([start_vector], axis=0)\n",
    "    caption_words = ''\n",
    "    \n",
    "    while True :\n",
    "        model.reset_states() # Reset les états de la LSTM\n",
    "        caption_vector = np.array(caption)\n",
    "        result = model.predict([np.array([processed_image]), caption_vector], verbose=0)[0]\n",
    "        result_vector = find_closest_vector(result, vec)\n",
    "\n",
    "        if np.array_equal(result_vector, vec[END_TOKEN]):\n",
    "            break\n",
    "        else:\n",
    "            caption_words += find_closest_word(result_vector, vec) + ' '\n",
    "            result_vector = np.expand_dims(result_vector, axis=0)\n",
    "            caption = np.concatenate([caption, [result_vector]], axis=1)\n",
    "        if len(caption_words.split()) >= MAX_LEN_SEQUENCE :\n",
    "            break\n",
    "    return caption_words\n",
    "\n",
    "def caption_model_inferencev2(model,vec, image, temperature=1e-5):\n",
    "    \"\"\"\n",
    "    inputs :\n",
    "        model : model keras\n",
    "        word2vec : model word2vec\n",
    "        image : image (batch_size, 224, 224, 3) (RGB) (0,255)\n",
    "        caption : (batch_size, None, TEXT_VECTOR_SIZE)\n",
    "    outputs :\n",
    "        caption : (batch_size, VOCAB_SIZE)\n",
    "    \"\"\"\n",
    "    start_index = vec.key_to_index[START_TOKEN]\n",
    "    processed_image = preprocess_input(image.copy())\n",
    "    caption = np.expand_dims([start_index], axis=0)\n",
    "    caption_words = ''\n",
    "    \n",
    "    while True :\n",
    "        model.reset_states() # Reset les états de la LSTM\n",
    "        caption_tensor = np.array(caption)\n",
    "        logits = model.predict([np.array([processed_image]), caption_tensor], verbose=0)[0]\n",
    "        probabilities = tf.nn.softmax(logits / temperature).numpy()  # Appliquer la température\n",
    "        index_result = np.random.choice(np.arange(len(probabilities)), p=probabilities)  # Échantillonnage\n",
    "\n",
    "\n",
    "        if index_result == vec.key_to_index[END_TOKEN]:\n",
    "            break\n",
    "        else:\n",
    "            # On print le % de confiance du token <eos> pour voir si le modèle est confiant\n",
    "            #print(f' % de confiance pour le mot <eos> : {result[vec.key_to_index[END_TOKEN]]}')\n",
    "            caption_words += vec.index_to_key[index_result] + ' '\n",
    "            result_tensor = np.expand_dims(index_result, axis=0)\n",
    "            caption = np.concatenate([caption, [result_tensor]], axis=1)\n",
    "        if len(caption_words.split()) >= MAX_LEN_SEQUENCE :\n",
    "            break\n",
    "    return caption_words\n",
    "\n",
    "# Choix du générateur\n",
    "generator = val_generator\n",
    "# Test du modèle\n",
    "r_index = np.random.randint(len(generator))\n",
    "x, _ = generator.__getitem__(r_index)\n",
    "images, _ = x\n",
    "r_index = np.random.randint(0, images.shape[0]-1)\n",
    "selected_image = images[r_index]\n",
    "# On recentre les valeurs de l'image\n",
    "selected_image = ( selected_image - np.min(selected_image) ) / ( np.max(selected_image) - np.min(selected_image) ) * 255\n",
    "# On convertit l'image en RGB pour l'affichage\n",
    "selected_image = np.array(selected_image)\n",
    "selected_image = selected_image.astype('uint8')\n",
    "selected_image = selected_image[...,::-1]\n",
    "\n",
    "# Prédiction de la légende\n",
    "predicted_caption = caption_model_inferencev2(model, vec, selected_image)\n",
    "\n",
    "# Plot de l'image avec la légende prédite\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(selected_image)\n",
    "plt.title(predicted_caption)\n",
    "plt.axis('off')  # Désactiver les axes pour une meilleure visibilité\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
